{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0eb47fc",
   "metadata": {},
   "source": [
    "### Introduction to PyArrow\n",
    "\n",
    "*   PyArrow serves as a cross-language development environment specifically designed for in-memory data.\n",
    "*   Its primary goal is to boost the performance of analytics applications.\n",
    "*   Emerging from the Apache Arrow project, PyArrow aims to make data interoperability better across different languages and systems.\n",
    "*   It uses an in-memory columnar data representation, offering an optimized memory footprint for complex data structures.\n",
    "*   With zero-copy reads, it facilitates quick data sharing between Python and other languages, sidestepping the need for serialization.\n",
    "*   It supports schemas and metadata, providing data structures that are rich and self-describing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3013192",
   "metadata": {},
   "source": [
    "### PyArrow and Parquet\n",
    "\n",
    "*   PyArrow offers seamless reading and writing operations for Parquet files.\n",
    "*   With column pruning, you can selectively read only the necessary columns from a Parquet file, reducing I/O time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq \n",
    "table = pq.read_table('your_file.parquet', columns=['column1', 'column2']) \n",
    "# Potentially conver the file to pandas if needed for more sophisticated splicing and dicing.\n",
    "df = table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46b332",
   "metadata": {},
   "source": [
    "### Apache Arrow\n",
    "\n",
    "```The core feature of Apache Arrow is its in-memory columnar format. This language-agnostic standard is designed to store structured, table-like datasets efficiently in memory. The data format supports a rich set of data types, including nested and user-defined types, making it suitable for analytic databases, data frame libraries, and more.``` \n",
    "\n",
    "The Apache Arrow Project\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebcdbec",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://blog.djnavarro.net/posts/2021-11-19_starting-apache-arrow-in-r/img/with_arrow.jpg\" width=700>\n",
    "</div>\n",
    "\n",
    "[picture source](https://blog.djnavarro.net/posts/2021-11-19_starting-apache-arrow-in-r/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyarrow`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac5bc2",
   "metadata": {},
   "source": [
    "### PyArrow Data Structures\n",
    "\n",
    "*   PyArrow offers a suite of low-level data structures and methods optimized for both speed and flexibility.\n",
    "*   These structures can be used seamlessly across multiple languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57327800",
   "metadata": {},
   "source": [
    "### Arrow Array\n",
    "\n",
    "*   An Arrow Array is essentially a column of data stored in an efficient, contiguous block of memory.\n",
    "*   Unlike Python lists, these arrays are optimized for high-speed operations and can be transferred across languages without incurring serialization costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22df9c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "arrow_array = pa.array([1, 2, 3, 4, 5])\n",
    "print(type(arrow_array))\n",
    "print(\"---------\")\n",
    "print(arrow_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bcda45",
   "metadata": {},
   "source": [
    "### Arrow Buffer\n",
    "\n",
    "* While not a data structure per se, Arrow Buffers are pivotal in understanding Arrow functionality.\n",
    "* Buffers are blocks of memory that house the data for Arrow Arrays, contributing to efficient storage.\n",
    "* You can even access the buffer's content directly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35754c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = arrow_array.buffers()[1]\n",
    "print(buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c81af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_data = buffer.to_pybytes()\n",
    "print(byte_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed66a0",
   "metadata": {},
   "source": [
    "### Arrow Buffer - Cont'd\n",
    "\n",
    "* Here, the buffer's data contains 40 bytes, each 8 bytes representing an `int64` value for each of the 5 elements in the array.\n",
    "* You can use this buffer data to create a new NumPy array, showing that Arrow and NumPy can share memory.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d462d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "numpy_array = np.frombuffer(buffer, dtype=np.int64)\n",
    "numpy_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983cb2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shares_memory(arrow_array, numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac880c9",
   "metadata": {},
   "source": [
    "### Arrow Buffer - Cont'd\n",
    "\n",
    "* Both `arrow_array` and `numpy_array` share the same underlying data, demonstrating the concept of zero-copy.\n",
    "* You can confirm this by modifying a value in one array and seeing the change in the other.\n",
    "  * Both arrays will now show the updated value.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6df885",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array[1] = 0\n",
    "numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78887976",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b45749",
   "metadata": {},
   "source": [
    "### Schema\n",
    "\n",
    "* A schema in PyArrow defines the structure, column names, and types for Arrow Arrays.\n",
    "* Schemas are crucial as they set the framework for data manipulation and operations in Arrow.\n",
    "  * Give Arrow an idea on how to encode the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = pa.schema([('column1', pa.int64()), ('column2', pa.string())])\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd46394",
   "metadata": {},
   "source": [
    "### Chunked Array\n",
    "\n",
    "*   A Chunked Array in PyArrow is like a single Arrow Array but divided into smaller \"chunks.\"\n",
    "*   This structure allows for the storage and processing of datasets that are too large to fit in memory.\n",
    "*   It's commonly used in distributed computing frameworks and data streaming scenarios.\n",
    "\n",
    "* For example:\n",
    "  * you could have data sent in chunks to optimize throughput\n",
    "  * you might have multiple nodes in a distributed system each producing Arrow Arrays that are collected and represented as a ChunkedArray by the master node.\n",
    "\n",
    "* From a user perspective, a Chunked Array appears as a contiguous sequence of data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_node_1 = pa.array([0,1,2,3,4])\n",
    "results_node_2 = pa.array([5,6,7,8,9,10])\n",
    "chunked_array = pa.chunked_array([results_node_1, results_node_2])\n",
    "chunked_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be520fe2",
   "metadata": {},
   "source": [
    "### Chunked Array - Cont'd\n",
    "\n",
    "* You can index into a single position or even across multiple chunks, making the data handling more versatile.\n",
    "* You can also access individual chunks, allowing for parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d579f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_array[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356cedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_array.chunk(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948c663",
   "metadata": {},
   "source": [
    "### Table\n",
    "\n",
    "* A Table in PyArrow is a container for multiple Arrow ChunkedArrays with a common schema.\n",
    "* Each column in the Table is an Arrow ChunkedArray, and all columns share the same length.\n",
    "* Tables offer an ideal format for handling data in the form of a dataframe.\n",
    "* Tables can also be partitioned across multiple files for large-scale storage, or to be sent across a network, or even to be stored in-memory on a single machine.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "column1 = pa.array([0, 1, 2, 3, 4]) \n",
    "column2 = pa.array(['a', 'b', 'c', 'd', 'e'])\n",
    "table = pa.table({'column1': column1, 'column2': column2})  \n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdaa066",
   "metadata": {},
   "source": [
    "### Record Batch\n",
    "\n",
    "*   A Record Batch is a collection of Arrow Arrays (columns) with the same length, all of which are bundled together with a schema.\n",
    "*   Much like a Chunked Array is a collection of Arrow Arrays, a Table in Apache Arrow is a collection of Record Batches.\n",
    "\n",
    "* Conceptual Relationship\n",
    "  *   In Apache Arrow, the concept of a Record Batch is to a Table what an Arrow Array is to a Chunked Array.\n",
    "    *   Arrays can be grouped together to form a Chunked Array.\n",
    "    *   Record Batches can be grouped together to form a Table.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec7454b",
   "metadata": {},
   "source": [
    "### Record Batch - Cont'd\n",
    "\n",
    "* Use Cases\n",
    "  *   The choice between using a Record Batch or a Table often depends on your specific needs. E.g.:\n",
    "    \n",
    "  *  Streaming Data: If you need to process data on-the-fly, perhaps in a streaming application where you want to process each chunk as it arrives, Record Batches are a good choice.\n",
    "    *   You can serialize and process each Record Batch independently as they arrive, without having to wait for the entire data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column1_array = pa.array([1, 2, 3, 4, 5])\n",
    "column2_array = pa.array(['a', 'b', 'c', 'd', 'e'])\n",
    "schema = pa.schema([('column1', pa.int64()), ('column2', pa.string())])\n",
    "\n",
    "record_batch = pa.record_batch([column1_array, column2_array], schema=schema)\n",
    "record_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18702115",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe3e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch[\"column1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f94f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column1_array_new = pa.array([6, 7, 8, 9, 10])\n",
    "column2_array_new = pa.array(['f', 'g', 'h', 'i', 'j'])\n",
    "record_batch_new = pa.record_batch([column1_array_new, column2_array_new], schema=schema)\n",
    "\n",
    "\n",
    "table = pa.Table.from_batches([record_batch, record_batch_new], schema=schema)\n",
    "table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf06e41",
   "metadata": {},
   "source": [
    "### Record Batch - Cont'd\n",
    "\n",
    "* In the example above, two Record Batches are combined to create a single Table. \n",
    "  * This is analogous to how individual Arrow Arrays can be combined to create a Chunked Array\n",
    "  * Reinforces the idea that a Record Batch is to a Table what an Arrow Array is to a Chunked Array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede8f6b",
   "metadata": {},
   "source": [
    "### Dive Into Real Data: Parquet and Memory Efficiency\n",
    "\n",
    "1.  Let's get hands-on and read a Parquet file using Apache Arrow.\n",
    "2.  Take note: the size of the data when using PyArrow is substantially smaller than a Pandas DataFrame for the same data.\n",
    "3.  Think of this as a little teaser to whet your appetite for data science goodness.\n",
    "\n",
    "**Note**: Here, I'm using the `parquet` module from the PyArrow package. This module knows how to read Parquet files among other things.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaeb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "table = pq.read_table('/Users/mahdi/Downloads/fhvhv_tripdata_2022-06.parquet')\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e46add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(table) / 1024 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4504cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import psutil\n",
    "# def print_mem():\n",
    "#     gig = psutil.Process(os.getpid()).memory_info().rss / 1024 ** 3\n",
    "#     print(f\"{gig} gigabytes\")\n",
    "\n",
    "# print_mem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('/Users/mahdi/Downloads/fhvhv_tripdata_2022-06.parquet')\n",
    "sys.getsizeof(df) / 1024 / 1024 / 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f4201f",
   "metadata": {},
   "source": [
    "\n",
    "### Apache Arrow Datasets\n",
    "\n",
    "\n",
    "*   Datasets in PyArrow let you work with large tabular data, even when it's larger than your machine's memory\n",
    "*   It offers lazy data access, meaning you don't have to load the entire dataset into memory.\n",
    "*   Datasets support data discovery, partitioning, and compatibility with various file systems like AWS, Google Cloud, and local storage.\n",
    "  * I can read from AWS or Google without having to install anything.\n",
    "\n",
    "* import the dataset library as:\n",
    "\n",
    "```python\n",
    "import pyarrow.dataset as ds\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50e765",
   "metadata": {},
   "source": [
    "### Dataset Overview\n",
    "\n",
    "* Provider: New York City Taxi and Limousine Commission (TLC)\n",
    "* Data hosted on AWS. The URSA-LAB company account.\n",
    "* Contains data on millions of taxi and limousine trips in NYC\n",
    "* Time Period: 2009 to 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b42c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Note**: In the AWS S3 listing, \"PRE\" stands for \"prefix,\" essentially representing a folder or directory.\n",
    "\n",
    "!aws s3 ls \"s3://ursa-labs-taxi-data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c3ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls \"s3://ursa-labs-taxi-data/2009/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da9fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pyarrow.dataset as ds\n",
    "dataset = ds.dataset(\"s3://ursa-labs-taxi-data/\", partitioning=[\"year\", \"month\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60be24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.files[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how to load just one file (a fragment) and its schema:\n",
    "\n",
    "frag = next(dataset.get_fragments())\n",
    "frag.partition_expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d98407",
   "metadata": {},
   "source": [
    "#### Play with a Single File\n",
    "\n",
    "* Let's read in the data from this single fragment\n",
    "* Take a look at the data\n",
    "* List of column names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4124c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "frag_table = frag.to_table()\n",
    "frag_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed59a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_table.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_table.num_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626148d",
   "metadata": {},
   "source": [
    "#### Chunks: The Building Blocks\n",
    "\n",
    "* Remember how we talked about Arrow tables having columns that could be split into chunks? \n",
    "* If you take a look, each column is divided into 216 chunks\n",
    "  * Proving that this table is built in the way we discussed earlier.\n",
    "* Take just a slice of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_table.slice(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[frag_table[col_name].num_chunks for col_name in frag_table.column_names]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3e6dd",
   "metadata": {},
   "source": [
    "### The Essentials of Apache Arrow Tables and Record Batches\n",
    "\n",
    "*  Tables in Apache Arrow are essentially collections of record batches.\n",
    "*  You can easily pull data from columns like `payment_type`, `fare_amount`, or `tip_amount`. \n",
    "* Because we're working with a single record batch, managing the data is pretty straightforward. \n",
    "  * We'll see that each column, for instance, holds 65,536 values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf90cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch_3 = frag_table.to_batches()[3]\n",
    "record_batch_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cbb487",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch_3.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d7fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch_3[\"fare_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0735bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch_3['tip_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8074fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch_3['payment_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75535c",
   "metadata": {},
   "source": [
    "#### PyArrow's Computational Capabilities\n",
    "\n",
    "*   PyArrow separates data storage concerns from computational functionality.    \n",
    "    * Structures like Arrow Arrays, Record Batches, and Tables handle data storage and serialization.\n",
    "    * For actual data operations, there's the `pyarrow.compute` module.\n",
    "*   The `pyarrow.compute` module offers a range of functions for filtering, transforming, and aggregating data.    \n",
    "    * While it does provide useful operations, it's not a full-blown analytical tool. \n",
    "    * For more complex tasks, you'd typically use something like Pandas or Spark.\n",
    "\n",
    "* Let's perform some computations like calculating the sum of tips and fares, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.compute as pc\n",
    "pc.add(record_batch_3['tip_amount'], record_batch_3['fare_amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0365b652",
   "metadata": {},
   "source": [
    "* How about finding the maximum total amount for a trip, including the tip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.max(pc.add(record_batch_3['tip_amount'], record_batch_3['fare_amount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e0bcf",
   "metadata": {},
   "source": [
    "* And the average?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.mean(pc.add(record_batch_3['tip_amount'], record_batch_3['fare_amount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a8057",
   "metadata": {},
   "source": [
    "* We can also perform operations on string data, like converting the case of `payment_type`, which has been recorded inconsistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_cased_payment_type = pc.utf8_upper(record_batch_3[\"payment_type\"])\n",
    "upper_cased_payment_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3597a",
   "metadata": {},
   "source": [
    "* You can then filter data based on whether the payment type was \"CASH.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c334143",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cash = pc.equal(upper_cased_payment_type, pa.scalar('CASH'))\n",
    "is_cash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff03b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_record_batch_3 = pc.filter(record_batch_3, is_cash)\n",
    "filtered_record_batch_3\n",
    "filtered_record_batch_3.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7912d0",
   "metadata": {},
   "source": [
    "\n",
    "#### Working with Parquet Files\n",
    "\n",
    "* You can read Parquet data into PyArrow as a ParquetDataset, and then work with it as ParquetFile Fragments.\n",
    "* Recall that: \n",
    "    * Each fragment has its own metadata, \n",
    "    * You can also get statistics about each row group within the fragment.\n",
    "      * However, it's usually more efficient to work with sorted data if you carry out frequent operations\n",
    "      * You can then save this sorted table into a new Parquet file for optimized data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e6667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa \n",
    "import pyarrow.parquet as pq\n",
    "dataset = pq.ParquetDataset('s3://ursa-labs-taxi-data/2009/', partitioning=[\"month\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13072afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "data_table = dataset.fragments[0].to_table() \n",
    "sorted_indices = pc.sort_indices(data_table, sort_keys=[(\"dropoff_at\", \"ascending\"), (\"fare_amount\", \"ascending\")])\n",
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af0862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the instances in the order specified in the variable sorted_indices\n",
    "# i.e., sorting the data\n",
    "sorted_table = data_table.take(sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ee6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pq.write_table(sorted_table, 'optimized_parquet_file.parquet', row_group_size=65536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33953a7",
   "metadata": {},
   "source": [
    "\n",
    "#### Exploring Sorted Parquet Files\n",
    "\n",
    "*   When you read the sorted table back into PyArrow, it's easier to work with.\n",
    "  * We can reach the read groups meta data and only look at those we are interested in.\n",
    "  * i.e., you can delve into the metadata to understand your data better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836dc2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_parquet_file = pq.ParquetFile('optimized_parquet_file.parquet')\n",
    "rg0_metadata = optimized_parquet_file.metadata.row_group(0)\n",
    "rg0_metadata_dict = rg0_metadata.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,x[\"path_in_schema\"]) for i, x in enumerate(rg0_metadata.to_dict()[\"columns\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7785bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_2_pos = {x[\"path_in_schema\"]:i for i, x in enumerate(rg0_metadata.to_dict()[\"columns\"])}\n",
    "name_2_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61055ae7",
   "metadata": {},
   "source": [
    "# Bonus Question Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc677fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa \n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d2c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_parquet_file = pq.ParquetFile('optimized_parquet_file.parquet')\n",
    "rg0_metadata = optimized_parquet_file.metadata.row_group(0)\n",
    "name_2_pos = {x[\"path_in_schema\"]:i for i, x in enumerate(rg0_metadata.to_dict()[\"columns\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a464a355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start point (2:00 PM) found in row_group 2\n",
      "Ending point (2:59 PM) found in row_group 2\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "col_idx = name_2_pos['dropoff_at']\n",
    "\n",
    "datetime_obj_start = datetime.strptime(\"2009-1-1 14:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "datetime_obj_end = datetime.strptime(\"2009-1-1 14:59:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "for i in range(optimized_parquet_file.num_row_groups):\n",
    "    col_stats = optimized_parquet_file.metadata.row_group(i).column(col_idx).statistics    \n",
    "    if col_stats.min <= datetime_obj_start <= col_stats.max:\n",
    "        print(f\"Start point (2:00 PM) found in row_group {i}\")\n",
    "        \n",
    "    if col_stats.min <= datetime_obj_end <= col_stats.max:\n",
    "        print(f\"Ending point (2:59 PM) found in row_group {i}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507c394c",
   "metadata": {},
   "source": [
    "### Bonus Question 1\n",
    "    *  can you get the average transaction between 2:00-2:59 PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c81737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering \"Dropoff_at\" instead of \"pickup_at\" as the transaction takes place after the dropoff\n",
    "\n",
    "rg2 = optimized_parquet_file.read_row_group(2)\n",
    "\n",
    "#Drop-off time >= 2:00 PM\n",
    "gr_2_00PM = pc.greater_equal(rg2['dropoff_at'], datetime_obj_start)\n",
    "\n",
    "#Drop-off time <= 2:59 PM\n",
    "ls_2_59PM = pc.less_equal(rg2['dropoff_at'], datetime_obj_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc31eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the data based on the above two conditions\n",
    "all_drop_off_in_range = pc.filter(rg2, pc.and_(gr_2_00PM, ls_2_59PM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a489c392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x12f63f380>\n",
       "[\n",
       "  [\n",
       "    2.5,\n",
       "    2.9,\n",
       "    3.3,\n",
       "    3.3,\n",
       "    3.3,\n",
       "    ...\n",
       "    49.15,\n",
       "    49.15,\n",
       "    49.15,\n",
       "    45,\n",
       "    47.7\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_drop_off_in_range['total_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26735764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.DoubleScalar: 10.370755473289336>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.mean(all_drop_off_in_range['total_amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea2005",
   "metadata": {},
   "source": [
    "    average transaction between 2:00-2:59 PM on 2009-1-1 was $10.37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d50dccb",
   "metadata": {},
   "source": [
    "### Bonus Question 2\n",
    "        * Which day, on average has the highest tip? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "910d2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74999209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/216 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:20<00:00, 10.46it/s]\n"
     ]
    }
   ],
   "source": [
    "tip_per_day = defaultdict(float)\n",
    "count_per_day = defaultdict(int)\n",
    "\n",
    "# loop over all row groups\n",
    "for i in tqdm(range(optimized_parquet_file.num_row_groups)):\n",
    "\n",
    "    #convert each row group to pandas dataframe\n",
    "    rg = optimized_parquet_file.read_row_group(i).to_pandas()\n",
    "    # convert the pickup_at column to string and extract the date in yyyy-mm-dd format \n",
    "    rg['pickup_at'] = rg['pickup_at'].astype('str')\n",
    "    rg['pickup_at'] = rg['pickup_at'].str.split(' ', expand=True)[0]\n",
    "    \n",
    "    # sum all the tips for each day and count the number of trips for each day\n",
    "    total_tip = rg.groupby('pickup_at')['tip_amount'].sum()\n",
    "    counts = rg.groupby('pickup_at')['pickup_at'].count()\n",
    "\n",
    "    # add the tip and count for each day to the defaultdict\n",
    "    for date, tip in total_tip.items():\n",
    "        tip_per_day[date] += tip\n",
    "        count_per_day[date] += counts[date]\n",
    "\n",
    "# round the total to 2 decimal places\n",
    "tip_per_day = {k: round(v, 2) for k, v in tip_per_day.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dacbd810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the avaerage tip for each day\n",
    "avg = {}\n",
    "for k, v in tip_per_day.items():\n",
    "    avg[k] = round(v / count_per_day[k], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3048381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the avg by values in descending order to get the max avg tip\n",
    "# Can be done efficiently without sorting, but this was done just for curiosity\n",
    "sorted_avg = sorted(avg.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05a1cf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tsorted by avg tip per day\n",
      "\n",
      "Date\t\t|\tTotal\t\t|\tCount\t|\tAverage\n",
      "------------------------------------------------------------------------\n",
      "2009-01-29\t|\t267373.54\t|\t503352\t|\t0.53\n",
      "2009-01-15\t|\t251352.51\t|\t486450\t|\t0.52\n",
      "2009-01-21\t|\t242664.74\t|\t478268\t|\t0.51\n",
      "2009-01-22\t|\t255874.08\t|\t498241\t|\t0.51\n",
      "2009-01-28\t|\t233095.36\t|\t455187\t|\t0.51\n",
      "2009-01-14\t|\t246874.6\t|\t489177\t|\t0.5\n",
      "2009-01-20\t|\t216461.05\t|\t433639\t|\t0.5\n",
      "2009-01-27\t|\t235145.65\t|\t471788\t|\t0.5\n",
      "2009-01-30\t|\t267957.11\t|\t538365\t|\t0.5\n",
      "2009-01-13\t|\t216002.36\t|\t442543\t|\t0.49\n",
      "2009-01-19\t|\t173412.32\t|\t352534\t|\t0.49\n",
      "2009-01-23\t|\t254119.05\t|\t519394\t|\t0.49\n",
      "2009-01-16\t|\t259469.1\t|\t535200\t|\t0.48\n",
      "2009-01-25\t|\t220655.79\t|\t460723\t|\t0.48\n",
      "2009-01-26\t|\t210454.43\t|\t434081\t|\t0.48\n",
      "2009-01-11\t|\t191854.23\t|\t405075\t|\t0.47\n",
      "2009-01-12\t|\t195425.52\t|\t414642\t|\t0.47\n",
      "2009-01-08\t|\t220165.2\t|\t477502\t|\t0.46\n",
      "2009-01-04\t|\t166291.08\t|\t367525\t|\t0.45\n",
      "2009-01-07\t|\t168789.04\t|\t371043\t|\t0.45\n",
      "2009-01-09\t|\t232536.04\t|\t520846\t|\t0.45\n",
      "2009-01-18\t|\t189843.75\t|\t419962\t|\t0.45\n",
      "2009-01-05\t|\t162921.39\t|\t370901\t|\t0.44\n",
      "2009-01-06\t|\t185202.15\t|\t427394\t|\t0.43\n",
      "2009-01-24\t|\t233566.28\t|\t547591\t|\t0.43\n",
      "2009-01-31\t|\t234567.14\t|\t539574\t|\t0.43\n",
      "2009-01-17\t|\t212806.78\t|\t511023\t|\t0.42\n",
      "2009-01-10\t|\t196560.61\t|\t483350\t|\t0.41\n",
      "2009-01-03\t|\t158717.59\t|\t432710\t|\t0.37\n",
      "2009-01-02\t|\t128045.42\t|\t376708\t|\t0.34\n",
      "2009-01-01\t|\t108082.3\t|\t327625\t|\t0.33\n"
     ]
    }
   ],
   "source": [
    "print('\\t\\t\\tsorted by avg tip per day\\n')\n",
    "\n",
    "print(f'Date\\t\\t|\\tTotal\\t\\t|\\tCount\\t|\\tAverage')\n",
    "print('------------------------------------------------------------------------')\n",
    "for row in sorted_avg:\n",
    "    print(f'{row[0]}\\t|\\t{tip_per_day[row[0]]}\\t|\\t{count_per_day[row[0]]}\\t|\\t{row[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb5d3e",
   "metadata": {},
   "source": [
    "    On Average 2009-01-29 had the most ammount of tip ($0.53/trip) and 2009-01-01 had the lowest ($0.32/trip) in the month of January in 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c2f721",
   "metadata": {},
   "source": [
    "### Bonus Question 3\n",
    "        * Which time (hour) of the day has the highest tip?\n",
    "        Might be interpreted in 3 different ways.\n",
    "                1. Generally Which hour of the days brought in more tip in January, 2009\n",
    "                2. Consider it linked with Question 2: \n",
    "                    Which hour of the 2009-01-29 brought in most of the tip\n",
    "                3. Consider it linked with Question 1: \n",
    "                    Which hour of the 2009-01-1 brought in most of the tip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a65bdd5",
   "metadata": {},
   "source": [
    "    1. Generally Which hour of the days brought in more tips in January, 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "105bef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:45<00:00,  4.75it/s]\n"
     ]
    }
   ],
   "source": [
    "tips_bucket = {}\n",
    "count_bucket = {}\n",
    "\n",
    "for i in tqdm(range(optimized_parquet_file.num_row_groups)):\n",
    "    rg = optimized_parquet_file.read_row_group(i).to_pandas()\n",
    "    # convert the pickup_at column to string and extract the date in yyyy-mm-dd format \n",
    "    rg['pickup_at'] = rg['pickup_at'].astype('str')\n",
    "    rg['pickup_at_hour'] = rg['pickup_at'].str.split(' ', expand=True)[1]\n",
    "    rg['pickup_at_hour'] = rg['pickup_at_hour'].str.split(':', expand=True)[0].astype('int')\n",
    "    rg['pickup_at'] = rg['pickup_at'].str.split(' ', expand=True)[0]\n",
    "    \n",
    "    # group by the hour and find the max tip for each hour\n",
    "    group = rg.groupby('pickup_at_hour')['tip_amount'].max().sort_values(ascending=False)\n",
    "    count = rg.groupby('pickup_at_hour')['pickup_at_hour'].count()\n",
    "    for key in group.index:\n",
    "        if key not in tips_bucket.keys():\n",
    "            tips_bucket[key] = group[key]\n",
    "            count_bucket[key] = count[key]\n",
    "        else:\n",
    "            tips_bucket[key] += group[key]\n",
    "            count_bucket[key] += count[key]\n",
    "\n",
    "\n",
    "tips_bucket = {k: round(v, 2) for k, v in tips_bucket.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4582574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the avaerage tip for each day\n",
    "avg_tips_hour = {}\n",
    "for k, v in tips_bucket.items():\n",
    "    avg_tips_hour[k] = round((v / count_bucket[k])*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83ae10c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_avg_hour = sorted(avg_tips_hour.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_tips_bucket = sorted(tips_bucket.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2abb83d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 2986.03),\n",
       " (14, 2790.38),\n",
       " (12, 2765.73),\n",
       " (16, 2743.74),\n",
       " (22, 2668.85),\n",
       " (20, 2668.47),\n",
       " (0, 2597.99),\n",
       " (18, 2573.61),\n",
       " (19, 2540.96),\n",
       " (17, 2519.28),\n",
       " (21, 2511.47),\n",
       " (23, 2417.89),\n",
       " (13, 2394.76),\n",
       " (9, 2393.78),\n",
       " (7, 2250.11),\n",
       " (8, 2219.56),\n",
       " (1, 2217.7),\n",
       " (10, 2163.76),\n",
       " (11, 2146.86),\n",
       " (3, 1896.05),\n",
       " (4, 1871.07),\n",
       " (2, 1685.02),\n",
       " (6, 1650.61),\n",
       " (5, 1596.38)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_tips_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "563a844b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tsorted by avg tip per hour\n",
      "\n",
      "Hour\t|\tTotal\t|\tCount\t|\tAverage\n",
      "------------------------------------------------------------------------\n",
      "5\t|\t1596.38\t|\t123974\t|\t1.29\n",
      "4\t|\t1871.07\t|\t160377\t|\t1.17\n",
      "3\t|\t1896.05\t|\t225803\t|\t0.84\n",
      "6\t|\t1650.61\t|\t265589\t|\t0.62\n",
      "1\t|\t2217.70\t|\t394213\t|\t0.56\n",
      "2\t|\t1685.02\t|\t303536\t|\t0.56\n",
      "0\t|\t2597.99\t|\t529466\t|\t0.49\n",
      "7\t|\t2250.11\t|\t485629\t|\t0.46\n",
      "15\t|\t2986.03\t|\t708964\t|\t0.42\n",
      "12\t|\t2765.73\t|\t671394\t|\t0.41\n",
      "16\t|\t2743.74\t|\t668310\t|\t0.41\n",
      "14\t|\t2790.38\t|\t702784\t|\t0.4\n",
      "9\t|\t2393.78\t|\t642209\t|\t0.37\n",
      "23\t|\t2417.89\t|\t676273\t|\t0.36\n",
      "10\t|\t2163.76\t|\t595728\t|\t0.36\n",
      "13\t|\t2394.76\t|\t674441\t|\t0.36\n",
      "11\t|\t2146.86\t|\t613942\t|\t0.35\n",
      "8\t|\t2219.56\t|\t640186\t|\t0.35\n",
      "22\t|\t2668.85\t|\t767651\t|\t0.35\n",
      "17\t|\t2519.28\t|\t766431\t|\t0.33\n",
      "21\t|\t2511.47\t|\t789349\t|\t0.32\n",
      "20\t|\t2668.47\t|\t830731\t|\t0.32\n",
      "18\t|\t2573.61\t|\t913010\t|\t0.28\n",
      "19\t|\t2540.96\t|\t942423\t|\t0.27\n"
     ]
    }
   ],
   "source": [
    "print('\\t\\t\\tsorted by avg tip per hour\\n')\n",
    "\n",
    "print(f'Hour\\t|\\tTotal\\t|\\tCount\\t|\\tAverage')\n",
    "print('------------------------------------------------------------------------')\n",
    "for row in sorted_avg_hour:\n",
    "    print(f'{row[0]}\\t|\\t{tips_bucket[row[0]]:.2f}\\t|\\t{count_bucket[row[0]]}\\t|\\t{row[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c045aa",
   "metadata": {},
   "source": [
    "    Through January, 2009 most tips (on average) were generated during the night - from 12 AM to 7 AM with 5AM having the most. \n",
    "    However if we consider the total amount of tips, the noon-time (12 PM to 4 PM) seems more generous with 3 PM having the most amount of tips.\n",
    "    -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24077c3a",
   "metadata": {},
   "source": [
    "    2. Which hour of the 2009-01-29 brought in most of the tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4b9b4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:52<00:00,  4.11it/s]\n"
     ]
    }
   ],
   "source": [
    "tips_bucket = {}\n",
    "count_bucket = {}\n",
    "\n",
    "for i in tqdm(range(optimized_parquet_file.num_row_groups)):\n",
    "    rg = optimized_parquet_file.read_row_group(i).to_pandas()\n",
    "    # convert the pickup_at column to string and extract the date in yyyy-mm-dd format \n",
    "    rg['pickup_at'] = rg['pickup_at'].astype('str')\n",
    "    rg['pickup_at_hour'] = rg['pickup_at'].str.split(' ', expand=True)[1]\n",
    "    rg['pickup_at_hour'] = rg['pickup_at_hour'].str.split(':', expand=True)[0].astype('int')\n",
    "    rg['pickup_at'] = rg['pickup_at'].str.split(' ', expand=True)[0]\n",
    "    rg = rg[rg['pickup_at'] == '2009-01-29']\n",
    "    \n",
    "    # group by the hour and find the max tip for each hour\n",
    "    group = rg.groupby('pickup_at_hour')['tip_amount'].max().sort_values(ascending=False)\n",
    "    count = rg.groupby('pickup_at_hour')['pickup_at_hour'].count()\n",
    "    for key in group.index:\n",
    "        if key not in tips_bucket.keys():\n",
    "            tips_bucket[key] = group[key]\n",
    "            count_bucket[key] = count[key]\n",
    "        else:\n",
    "            tips_bucket[key] += group[key]\n",
    "            count_bucket[key] += count[key]\n",
    "\n",
    "\n",
    "tips_bucket = {k: round(v, 2) for k, v in tips_bucket.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03ce8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the avaerage tip for each day\n",
    "avg_tips_hour = {}\n",
    "for k, v in tips_bucket.items():\n",
    "    avg_tips_hour[k] = round((v / count_bucket[k])*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd83aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_avg_hour = sorted(avg_tips_hour.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_tips_bucket = sorted(tips_bucket.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9850c367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 123.2),\n",
       " (8, 118.54),\n",
       " (22, 114.04),\n",
       " (19, 105.27),\n",
       " (15, 101.65),\n",
       " (12, 100.99),\n",
       " (7, 91.94),\n",
       " (20, 90.44),\n",
       " (9, 85.05),\n",
       " (14, 84.12),\n",
       " (17, 83.33),\n",
       " (18, 79.57),\n",
       " (10, 78.0),\n",
       " (23, 76.96),\n",
       " (13, 76.9),\n",
       " (16, 74.34),\n",
       " (21, 72.32),\n",
       " (11, 66.31),\n",
       " (1, 58.23),\n",
       " (6, 48.0),\n",
       " (3, 37.5),\n",
       " (5, 29.86),\n",
       " (2, 26.0),\n",
       " (4, 16.6)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_tips_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "33f323f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tsorted by avg tip per hour in 2009-01-29\n",
      "\n",
      "Hour\t|\tTotal\t|\tCount\t|\tAverage\n",
      "------------------------------------------------------------------------\n",
      "3\t|\t37.50\t|\t3202\t|\t1.17\n",
      "0\t|\t123.20\t|\t12450\t|\t0.99\n",
      "1\t|\t58.23\t|\t7322\t|\t0.8\n",
      "5\t|\t29.86\t|\t3813\t|\t0.78\n",
      "4\t|\t16.60\t|\t2549\t|\t0.65\n",
      "2\t|\t26.00\t|\t4648\t|\t0.56\n",
      "12\t|\t100.99\t|\t23529\t|\t0.43\n",
      "6\t|\t48.00\t|\t11504\t|\t0.42\n",
      "7\t|\t91.94\t|\t22543\t|\t0.41\n",
      "8\t|\t118.54\t|\t29231\t|\t0.41\n",
      "15\t|\t101.65\t|\t24679\t|\t0.41\n",
      "22\t|\t114.04\t|\t30689\t|\t0.37\n",
      "14\t|\t84.12\t|\t24318\t|\t0.35\n",
      "10\t|\t78.00\t|\t22844\t|\t0.34\n",
      "13\t|\t76.90\t|\t22505\t|\t0.34\n",
      "16\t|\t74.34\t|\t21584\t|\t0.34\n",
      "17\t|\t83.33\t|\t25994\t|\t0.32\n",
      "9\t|\t85.05\t|\t27205\t|\t0.31\n",
      "11\t|\t66.31\t|\t21904\t|\t0.3\n",
      "19\t|\t105.27\t|\t34945\t|\t0.3\n",
      "23\t|\t76.96\t|\t27584\t|\t0.28\n",
      "20\t|\t90.44\t|\t33386\t|\t0.27\n",
      "18\t|\t79.57\t|\t32552\t|\t0.24\n",
      "21\t|\t72.32\t|\t32372\t|\t0.22\n"
     ]
    }
   ],
   "source": [
    "print('\\t\\tsorted by avg tip per hour in 2009-01-29\\n')\n",
    "\n",
    "print(f'Hour\\t|\\tTotal\\t|\\tCount\\t|\\tAverage')\n",
    "print('------------------------------------------------------------------------')\n",
    "for row in sorted_avg_hour:\n",
    "    print(f'{row[0]}\\t|\\t{tips_bucket[row[0]]:.2f}\\t|\\t{count_bucket[row[0]]}\\t|\\t{row[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ecddc",
   "metadata": {},
   "source": [
    "    On 2009-01-29, on average most tips were geenrated at 3 AM. The night time (12 AM - 8 AM) was the best for Tips on average.\n",
    "    When considering the total, During the evening, midnight and the noon-time there were spikes in Tips, 12 AM being the best.\n",
    "    ------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f93a956",
   "metadata": {},
   "source": [
    "    3. Which hour of the 2009-01-01 brought in most of the tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f56d9c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:52<00:00,  4.13it/s]\n"
     ]
    }
   ],
   "source": [
    "tips_bucket = {}\n",
    "count_bucket = {}\n",
    "\n",
    "for i in tqdm(range(optimized_parquet_file.num_row_groups)):\n",
    "    rg = optimized_parquet_file.read_row_group(i).to_pandas()\n",
    "    # convert the pickup_at column to string and extract the date in yyyy-mm-dd format \n",
    "    rg['pickup_at'] = rg['pickup_at'].astype('str')\n",
    "    rg['pickup_at_hour'] = rg['pickup_at'].str.split(' ', expand=True)[1]\n",
    "    rg['pickup_at_hour'] = rg['pickup_at_hour'].str.split(':', expand=True)[0].astype('int')\n",
    "    rg['pickup_at'] = rg['pickup_at'].str.split(' ', expand=True)[0]\n",
    "    rg = rg[rg['pickup_at'] == '2009-01-01']\n",
    "    \n",
    "    # group by the hour and find the max tip for each hour\n",
    "    group = rg.groupby('pickup_at_hour')['tip_amount'].max().sort_values(ascending=False)\n",
    "    count = rg.groupby('pickup_at_hour')['pickup_at_hour'].count()\n",
    "    for key in group.index:\n",
    "        if key not in tips_bucket.keys():\n",
    "            tips_bucket[key] = group[key]\n",
    "            count_bucket[key] = count[key]\n",
    "        else:\n",
    "            tips_bucket[key] += group[key]\n",
    "            count_bucket[key] += count[key]\n",
    "\n",
    "\n",
    "tips_bucket = {k: round(v, 2) for k, v in tips_bucket.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "04dcd5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the avaerage tip for each day\n",
    "avg_tips_hour = {}\n",
    "for k, v in tips_bucket.items():\n",
    "    avg_tips_hour[k] = round((v / count_bucket[k])*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2228bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_avg_hour = sorted(avg_tips_hour.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_tips_bucket = sorted(tips_bucket.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a648b0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 121.56),\n",
       " (16, 115.0),\n",
       " (12, 113.23),\n",
       " (1, 101.23),\n",
       " (3, 67.56),\n",
       " (11, 66.7),\n",
       " (4, 65.49),\n",
       " (14, 62.0),\n",
       " (6, 58.88),\n",
       " (21, 56.66),\n",
       " (18, 53.95),\n",
       " (13, 53.5),\n",
       " (19, 53.25),\n",
       " (23, 49.29),\n",
       " (10, 42.66),\n",
       " (15, 41.0),\n",
       " (9, 40.0),\n",
       " (22, 39.25),\n",
       " (17, 34.18),\n",
       " (0, 33.8),\n",
       " (2, 30.0),\n",
       " (7, 25.0),\n",
       " (8, 25.0),\n",
       " (5, 23.52)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_tips_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a39528d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tsorted by avg tip per hour in 2009-01-01\n",
      "\n",
      "Hour\t|\tTotal\t|\tCount\t|\tAverage\n",
      "------------------------------------------------------------------------\n",
      "6\t|\t58.88\t|\t3682\t|\t1.6\n",
      "9\t|\t40.00\t|\t3289\t|\t1.22\n",
      "8\t|\t25.00\t|\t2602\t|\t0.96\n",
      "7\t|\t25.00\t|\t2899\t|\t0.86\n",
      "20\t|\t121.56\t|\t17102\t|\t0.71\n",
      "12\t|\t113.23\t|\t16523\t|\t0.69\n",
      "4\t|\t65.49\t|\t9619\t|\t0.68\n",
      "1\t|\t101.23\t|\t16990\t|\t0.6\n",
      "16\t|\t115.00\t|\t19221\t|\t0.6\n",
      "3\t|\t67.56\t|\t13307\t|\t0.51\n",
      "11\t|\t66.70\t|\t13366\t|\t0.5\n",
      "10\t|\t42.66\t|\t9610\t|\t0.44\n",
      "5\t|\t23.52\t|\t5391\t|\t0.44\n",
      "23\t|\t49.29\t|\t13494\t|\t0.37\n",
      "21\t|\t56.66\t|\t16847\t|\t0.34\n",
      "14\t|\t62.00\t|\t19283\t|\t0.32\n",
      "13\t|\t53.50\t|\t18082\t|\t0.3\n",
      "18\t|\t53.95\t|\t19860\t|\t0.27\n",
      "19\t|\t53.25\t|\t20140\t|\t0.26\n",
      "22\t|\t39.25\t|\t15975\t|\t0.25\n",
      "0\t|\t33.80\t|\t15386\t|\t0.22\n",
      "15\t|\t41.00\t|\t19809\t|\t0.21\n",
      "2\t|\t30.00\t|\t15585\t|\t0.19\n",
      "17\t|\t34.18\t|\t19563\t|\t0.17\n"
     ]
    }
   ],
   "source": [
    "print('\\t\\tsorted by avg tip per hour in 2009-01-01\\n')\n",
    "\n",
    "print(f'Hour\\t|\\tTotal\\t|\\tCount\\t|\\tAverage')\n",
    "print('------------------------------------------------------------------------')\n",
    "for row in sorted_avg_hour:\n",
    "    print(f'{row[0]}\\t|\\t{tips_bucket[row[0]]:.2f}\\t|\\t{count_bucket[row[0]]}\\t|\\t{row[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5206b2a5",
   "metadata": {},
   "source": [
    "    On 2009-01-01, on average most tips were geenrated at 6 AM. The morning (6 AM - 9 AM) was the best for Tips on average.\n",
    "    When considering the total, During the evening through midnight the Tips were the most with 8PM at the top. During the Noon (at 12 PM) there was a spike in tips as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d45c6c",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "1.  [Apache Arrow Homepage](https://arrow.apache.org/)\n",
    "2.  [PyArrow Documentation](https://arrow.apache.org/docs/python/)\n",
    "3.  [PyArrow GitHub Repository](https://github.com/apache/arrow/tree/master/python/pyarrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7262ab3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
