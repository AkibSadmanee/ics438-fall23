{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0eb47fc",
   "metadata": {},
   "source": [
    "### Introduction to PyArrow\n",
    "\n",
    "*   PyArrow serves as a cross-language development environment specifically designed for in-memory data.\n",
    "*   Its primary goal is to boost the performance of analytics applications.\n",
    "*   Emerging from the Apache Arrow project, PyArrow aims to make data interoperability better across different languages and systems.\n",
    "*   It uses an in-memory columnar data representation, offering an optimized memory footprint for complex data structures.\n",
    "*   With zero-copy reads, it facilitates quick data sharing between Python and other languages, sidestepping the need for serialization.\n",
    "*   It supports schemas and metadata, providing data structures that are rich and self-describing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3013192",
   "metadata": {},
   "source": [
    "### PyArrow and Parquet\n",
    "\n",
    "*   PyArrow offers seamless reading and writing operations for Parquet files.\n",
    "*   With column pruning, you can selectively read only the necessary columns from a Parquet file, reducing I/O time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9cd6488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq \n",
    "table = pq.read_table('your_file.parquet', columns=['column1', 'column2']) \n",
    "# Potentially conver the file to pandas if needed for more sophisticated splicing and dicing.\n",
    "df = table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46b332",
   "metadata": {},
   "source": [
    "### Apache Arrow\n",
    "\n",
    "```The core feature of Apache Arrow is its in-memory columnar format. This language-agnostic standard is designed to store structured, table-like datasets efficiently in memory. The data format supports a rich set of data types, including nested and user-defined types, making it suitable for analytic databases, data frame libraries, and more.``` \n",
    "\n",
    "The Apache Arrow Project\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebcdbec",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://blog.djnavarro.net/posts/2021-11-19_starting-apache-arrow-in-r/img/with_arrow.jpg\" width=700>\n",
    "</div>\n",
    "\n",
    "[picture source](https://blog.djnavarro.net/posts/2021-11-19_starting-apache-arrow-in-r/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyarrow`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac5bc2",
   "metadata": {},
   "source": [
    "### PyArrow Data Structures\n",
    "\n",
    "*   PyArrow offers a suite of low-level data structures and methods optimized for both speed and flexibility.\n",
    "*   These structures can be used seamlessly across multiple languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57327800",
   "metadata": {},
   "source": [
    "### Arrow Array\n",
    "\n",
    "*   An Arrow Array is essentially a column of data stored in an efficient, contiguous block of memory.\n",
    "*   Unlike Python lists, these arrays are optimized for high-speed operations and can be transferred across languages without incurring serialization costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22df9c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyarrow.lib.Int64Array'>\n",
      "---------\n",
      "[\n",
      "  1,\n",
      "  2,\n",
      "  3,\n",
      "  4,\n",
      "  5\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "arrow_array = pa.array([1, 2, 3, 4, 5])\n",
    "print(type(arrow_array))\n",
    "print(\"---------\")\n",
    "print(arrow_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bcda45",
   "metadata": {},
   "source": [
    "### Arrow Buffer\n",
    "\n",
    "* While not a data structure per se, Arrow Buffers are pivotal in understanding Arrow functionality.\n",
    "* Buffers are blocks of memory that house the data for Arrow Arrays, contributing to efficient storage.\n",
    "* You can even access the buffer's content directly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35754c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow.Buffer address=0x59974030140 size=40 is_cpu=True is_mutable=True>\n"
     ]
    }
   ],
   "source": [
    "buffer = arrow_array.buffers()[1]\n",
    "print(buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c81af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n"
     ]
    }
   ],
   "source": [
    "byte_data = buffer.to_pybytes()\n",
    "print(byte_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed66a0",
   "metadata": {},
   "source": [
    "### Arrow Buffer - Cont'd\n",
    "\n",
    "* Here, the buffer's data contains 40 bytes, each 8 bytes representing an `int64` value for each of the 5 elements in the array.\n",
    "* You can use this buffer data to create a new NumPy array, showing that Arrow and NumPy can share memory.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d462d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "numpy_array = np.frombuffer(buffer, dtype=np.int64)\n",
    "numpy_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "983cb2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shares_memory(arrow_array, numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac880c9",
   "metadata": {},
   "source": [
    "### Arrow Buffer - Cont'd\n",
    "\n",
    "* Both `arrow_array` and `numpy_array` share the same underlying data, demonstrating the concept of zero-copy.\n",
    "* You can confirm this by modifying a value in one array and seeing the change in the other.\n",
    "  * Both arrays will now show the updated value.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d6df885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 3, 4, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_array[1] = 0\n",
    "numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78887976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.Int64Array object at 0x105c51340>\n",
       "[\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  5\n",
       "]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrow_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b45749",
   "metadata": {},
   "source": [
    "### Schema\n",
    "\n",
    "* A schema in PyArrow defines the structure, column names, and types for Arrow Arrays.\n",
    "* Schemas are crucial as they set the framework for data manipulation and operations in Arrow.\n",
    "  * Give Arrow an idea on how to encode the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "186b4447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column1: int64\n",
      "column2: string\n"
     ]
    }
   ],
   "source": [
    "schema = pa.schema([('column1', pa.int64()), ('column2', pa.string())])\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd46394",
   "metadata": {},
   "source": [
    "### Chunked Array\n",
    "\n",
    "*   A Chunked Array in PyArrow is like a single Arrow Array but divided into smaller \"chunks.\"\n",
    "*   This structure allows for the storage and processing of datasets that are too large to fit in memory.\n",
    "*   It's commonly used in distributed computing frameworks and data streaming scenarios.\n",
    "\n",
    "* For example:\n",
    "  * you could have data sent in chunks to optimize throughput\n",
    "  * you might have multiple nodes in a distributed system each producing Arrow Arrays that are collected and represented as a ChunkedArray by the master node.\n",
    "\n",
    "* From a user perspective, a Chunked Array appears as a contiguous sequence of data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4c3087a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x1269cc3b0>\n",
       "[\n",
       "  [\n",
       "    0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4\n",
       "  ],\n",
       "  [\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_node_1 = pa.array([0,1,2,3,4])\n",
    "results_node_2 = pa.array([5,6,7,8,9,10])\n",
    "chunked_array = pa.chunked_array([results_node_1, results_node_2])\n",
    "chunked_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be520fe2",
   "metadata": {},
   "source": [
    "### Chunked Array - Cont'd\n",
    "\n",
    "* You can index into a single position or even across multiple chunks, making the data handling more versatile.\n",
    "* You can also access individual chunks, allowing for parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87d579f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x1269cc4a0>\n",
       "[\n",
       "  [\n",
       "    3,\n",
       "    4\n",
       "  ],\n",
       "  [\n",
       "    5\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_array[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "356cedc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.Int64Array object at 0x1268b69a0>\n",
       "[\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4\n",
       "]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_array.chunk(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948c663",
   "metadata": {},
   "source": [
    "### Table\n",
    "\n",
    "* A Table in PyArrow is a container for multiple Arrow ChunkedArrays with a common schema.\n",
    "* Each column in the Table is an Arrow ChunkedArray, and all columns share the same length.\n",
    "* Tables offer an ideal format for handling data in the form of a dataframe.\n",
    "* Tables can also be partitioned across multiple files for large-scale storage, or to be sent across a network, or even to be stored in-memory on a single machine.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98dc32df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "column1: int64\n",
       "column2: string\n",
       "----\n",
       "column1: [[0,1,2,3,4]]\n",
       "column2: [[\"a\",\"b\",\"c\",\"d\",\"e\"]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column1 = pa.array([0, 1, 2, 3, 4]) \n",
    "column2 = pa.array(['a', 'b', 'c', 'd', 'e'])\n",
    "table = pa.table({'column1': column1, 'column2': column2})  \n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdaa066",
   "metadata": {},
   "source": [
    "### Record Batch\n",
    "\n",
    "*   A Record Batch is a collection of Arrow Arrays (columns) with the same length, all of which are bundled together with a schema.\n",
    "*   Much like a Chunked Array is a collection of Arrow Arrays, a Table in Apache Arrow is a collection of Record Batches.\n",
    "\n",
    "* Conceptual Relationship\n",
    "  *   In Apache Arrow, the concept of a Record Batch is to a Table what an Arrow Array is to a Chunked Array.\n",
    "    *   Arrays can be grouped together to form a Chunked Array.\n",
    "    *   Record Batches can be grouped together to form a Table.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec7454b",
   "metadata": {},
   "source": [
    "### Record Batch - Cont'd\n",
    "\n",
    "* Use Cases\n",
    "  *   The choice between using a Record Batch or a Table often depends on your specific needs. E.g.:\n",
    "    \n",
    "  *  Streaming Data: If you need to process data on-the-fly, perhaps in a streaming application where you want to process each chunk as it arrives, Record Batches are a good choice.\n",
    "    *   You can serialize and process each Record Batch independently as they arrive, without having to wait for the entire data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5606ad83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.RecordBatch\n",
      "column1: int64\n",
      "column2: string\n"
     ]
    }
   ],
   "source": [
    "\n",
    "column1_array = pa.array([1, 2, 3, 4, 5])\n",
    "column2_array = pa.array(['a', 'b', 'c', 'd', 'e'])\n",
    "schema = pa.schema([('column1', pa.int64()), ('column2', pa.string())])\n",
    "\n",
    "record_batch = pa.record_batch([column1_array, column2_array], schema=schema)\n",
    "record_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18702115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyarrow.lib.Int64Array object at 0x1269a54c0>\n",
       " [\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5\n",
       " ],\n",
       " <pyarrow.lib.StringArray object at 0x1269a5100>\n",
       " [\n",
       "   \"a\",\n",
       "   \"b\",\n",
       "   \"c\",\n",
       "   \"d\",\n",
       "   \"e\"\n",
       " ]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_batch.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48fe3e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.Int64Array object at 0x1269b1fa0>\n",
       "[\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5\n",
       "]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_batch[\"column1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "359f94f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "column1: int64\n",
       "column2: string\n",
       "----\n",
       "column1: [[1,2,3,4,5],[6,7,8,9,10]]\n",
       "column2: [[\"a\",\"b\",\"c\",\"d\",\"e\"],[\"f\",\"g\",\"h\",\"i\",\"j\"]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "column1_array_new = pa.array([6, 7, 8, 9, 10])\n",
    "column2_array_new = pa.array(['f', 'g', 'h', 'i', 'j'])\n",
    "record_batch_new = pa.record_batch([column1_array_new, column2_array_new], schema=schema)\n",
    "\n",
    "\n",
    "table = pa.Table.from_batches([record_batch, record_batch_new], schema=schema)\n",
    "table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf06e41",
   "metadata": {},
   "source": [
    "### Record Batch - Cont'd\n",
    "\n",
    "* In the example above, two Record Batches are combined to create a single Table. \n",
    "  * This is analogous to how individual Arrow Arrays can be combined to create a Chunked Array\n",
    "  * Reinforces the idea that a Record Batch is to a Table what an Arrow Array is to a Chunked Array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede8f6b",
   "metadata": {},
   "source": [
    "### Dive Into Real Data: Parquet and Memory Efficiency\n",
    "\n",
    "1.  Let's get hands-on and read a Parquet file using Apache Arrow.\n",
    "2.  Take note: the size of the data when using PyArrow is substantially smaller than a Pandas DataFrame for the same data.\n",
    "3.  Think of this as a little teaser to whet your appetite for data science goodness.\n",
    "\n",
    "**Note**: Here, I'm using the `parquet` module from the PyArrow package. This module knows how to read Parquet files among other things.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adaeb94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "hvfhs_license_num: string\n",
       "dispatching_base_num: string\n",
       "originating_base_num: string\n",
       "request_datetime: timestamp[us]\n",
       "on_scene_datetime: timestamp[us]\n",
       "pickup_datetime: timestamp[us]\n",
       "dropoff_datetime: timestamp[us]\n",
       "PULocationID: int64\n",
       "DOLocationID: int64\n",
       "trip_miles: double\n",
       "trip_time: int64\n",
       "base_passenger_fare: double\n",
       "tolls: double\n",
       "bcf: double\n",
       "sales_tax: double\n",
       "congestion_surcharge: double\n",
       "airport_fee: double\n",
       "tips: double\n",
       "driver_pay: double\n",
       "shared_request_flag: string\n",
       "shared_match_flag: string\n",
       "access_a_ride_flag: string\n",
       "wav_request_flag: string\n",
       "wav_match_flag: string\n",
       "----\n",
       "hvfhs_license_num: [[\"HV0003\",\"HV0003\",\"HV0003\",\"HV0003\",\"HV0005\",...,\"HV0005\",\"HV0003\",\"HV0005\",\"HV0005\",\"HV0005\"],[\"HV0003\",\"HV0005\",\"HV0003\",\"HV0003\",\"HV0003\",...,\"HV0003\",\"HV0003\",\"HV0003\",\"HV0003\",\"HV0003\"],...,[\"HV0005\",\"HV0005\",\"HV0005\",\"HV0003\",\"HV0003\",...,\"HV0003\",\"HV0003\",\"HV0003\",\"HV0003\",\"HV0003\"],[\"HV0003\",\"HV0005\",\"HV0003\",\"HV0003\",\"HV0003\",...,\"HV0003\",\"HV0003\",\"HV0003\",\"HV0003\",\"HV0005\"]]\n",
       "dispatching_base_num: [[\"B03404\",\"B03404\",\"B03404\",\"B03404\",\"B03406\",...,\"B03406\",\"B03404\",\"B03406\",\"B03406\",\"B03406\"],[\"B03404\",\"B03406\",\"B03404\",\"B03404\",\"B03404\",...,\"B03404\",\"B03404\",\"B03404\",\"B03404\",\"B03404\"],...,[\"B03406\",\"B03406\",\"B03406\",\"B03404\",\"B03404\",...,\"B03404\",\"B03404\",\"B03404\",\"B03404\",\"B03404\"],[\"B03404\",\"B03406\",\"B03404\",\"B03404\",\"B03404\",...,\"B03404\",\"B03404\",\"B03404\",\"B03404\",\"B03406\"]]\n",
       "originating_base_num: [[\"B03404\",\"B03404\",\"B03404\",\"B03404\",null,...,null,\"B03404\",null,null,null],[\"B03404\",null,\"B03404\",\"B03404\",\"B03404\",...,\"B03404\",\"B03404\",\"B03404\",\"B03404\",\"B03404\"],...,[null,null,null,\"B03404\",\"B03404\",...,\"B03404\",\"B03404\",\"B03404\",\"B03404\",\"B03404\"],[\"B03404\",null,\"B03404\",\"B03404\",\"B03404\",...,\"B03404\",\"B03404\",\"B03404\",\"B03404\",null]]\n",
       "request_datetime: [[2022-06-01 00:15:35.000000,2022-06-01 00:39:04.000000,2022-06-01 00:27:53.000000,2022-06-01 00:48:15.000000,2022-06-01 00:04:51.000000,...,2022-06-01 09:27:01.000000,2022-06-01 09:10:29.000000,2022-06-01 08:51:09.000000,2022-06-01 09:14:34.000000,2022-06-01 09:39:53.000000],[2022-06-01 09:15:07.000000,2022-06-01 09:37:37.000000,2022-06-01 09:17:57.000000,2022-06-01 09:02:01.000000,2022-06-01 09:22:55.000000,...,2022-06-01 14:33:14.000000,2022-06-01 14:02:15.000000,2022-06-01 14:30:21.000000,2022-06-01 14:16:09.000000,2022-06-01 14:28:30.000000],...,[2022-06-30 16:03:07.000000,2022-06-30 16:23:58.000000,2022-06-30 16:48:38.000000,2022-06-30 16:18:52.000000,2022-06-30 16:37:05.000000,...,2022-06-30 21:47:53.000000,2022-06-30 21:10:29.000000,2022-06-30 21:16:48.000000,2022-06-30 21:28:51.000000,2022-06-30 21:43:34.000000],[2022-06-30 21:50:10.000000,2022-06-30 21:38:54.000000,2022-06-30 21:37:35.000000,2022-06-30 21:14:54.000000,2022-06-30 21:49:59.000000,...,2022-06-30 23:20:49.000000,2022-06-30 23:36:13.000000,2022-06-30 23:50:50.000000,2022-06-30 23:02:40.000000,2022-06-30 23:00:28.000000]]\n",
       "on_scene_datetime: [[2022-06-01 00:17:20.000000,2022-06-01 00:40:36.000000,2022-06-01 00:31:34.000000,2022-06-01 00:49:38.000000,null,...,null,2022-06-01 09:12:34.000000,null,null,null],[2022-06-01 09:15:46.000000,null,2022-06-01 09:21:35.000000,2022-06-01 09:05:21.000000,2022-06-01 09:25:11.000000,...,2022-06-01 14:34:13.000000,2022-06-01 14:10:04.000000,2022-06-01 14:35:24.000000,2022-06-01 14:18:34.000000,2022-06-01 14:29:51.000000],...,[null,null,null,2022-06-30 16:25:34.000000,2022-06-30 16:39:05.000000,...,2022-06-30 21:50:19.000000,2022-06-30 21:16:34.000000,2022-06-30 21:20:52.000000,2022-06-30 21:31:48.000000,2022-06-30 21:44:12.000000],[2022-06-30 21:54:20.000000,null,2022-06-30 21:38:44.000000,2022-06-30 21:16:41.000000,2022-06-30 21:54:09.000000,...,2022-06-30 23:24:23.000000,2022-06-30 23:39:12.000000,2022-06-30 23:55:11.000000,2022-06-30 23:04:58.000000,null]]\n",
       "pickup_datetime: [[2022-06-01 00:17:41.000000,2022-06-01 00:42:37.000000,2022-06-01 00:36:22.000000,2022-06-01 00:51:18.000000,2022-06-01 00:13:33.000000,...,2022-06-01 09:30:54.000000,2022-06-01 09:12:38.000000,2022-06-01 09:04:49.000000,2022-06-01 09:20:02.000000,2022-06-01 09:44:46.000000],[2022-06-01 09:17:46.000000,2022-06-01 09:38:31.000000,2022-06-01 09:22:00.000000,2022-06-01 09:05:31.000000,2022-06-01 09:26:04.000000,...,2022-06-01 14:35:32.000000,2022-06-01 14:10:14.000000,2022-06-01 14:36:04.000000,2022-06-01 14:18:43.000000,2022-06-01 14:32:11.000000],...,[2022-06-30 16:08:46.000000,2022-06-30 16:28:06.000000,2022-06-30 16:58:02.000000,2022-06-30 16:25:42.000000,2022-06-30 16:40:48.000000,...,2022-06-30 21:51:26.000000,2022-06-30 21:17:21.000000,2022-06-30 21:22:53.000000,2022-06-30 21:32:20.000000,2022-06-30 21:45:36.000000],[2022-06-30 21:56:21.000000,2022-06-30 21:41:17.000000,2022-06-30 21:39:34.000000,2022-06-30 21:18:28.000000,2022-06-30 21:55:55.000000,...,2022-06-30 23:24:43.000000,2022-06-30 23:39:20.000000,2022-06-30 23:57:12.000000,2022-06-30 23:06:44.000000,2022-06-30 23:03:06.000000]]\n",
       "dropoff_datetime: [[2022-06-01 00:25:41.000000,2022-06-01 00:56:32.000000,2022-06-01 00:45:31.000000,2022-06-01 01:11:15.000000,2022-06-01 00:17:27.000000,...,2022-06-01 09:41:04.000000,2022-06-01 09:14:46.000000,2022-06-01 09:16:44.000000,2022-06-01 09:34:10.000000,2022-06-01 10:01:02.000000],[2022-06-01 09:24:48.000000,2022-06-01 09:58:51.000000,2022-06-01 10:07:51.000000,2022-06-01 09:54:00.000000,2022-06-01 09:34:02.000000,...,2022-06-01 14:57:26.000000,2022-06-01 14:31:23.000000,2022-06-01 14:57:52.000000,2022-06-01 14:28:05.000000,2022-06-01 15:00:40.000000],...,[2022-06-30 16:19:05.000000,2022-06-30 16:45:30.000000,2022-06-30 17:25:43.000000,2022-06-30 16:36:19.000000,2022-06-30 16:48:40.000000,...,2022-06-30 21:58:39.000000,2022-06-30 21:43:31.000000,2022-06-30 21:30:31.000000,2022-06-30 21:43:01.000000,2022-06-30 21:52:27.000000],[2022-06-30 22:03:42.000000,2022-06-30 21:56:58.000000,2022-06-30 21:53:26.000000,2022-06-30 21:49:15.000000,2022-06-30 22:33:30.000000,...,2022-06-30 23:38:19.000000,2022-06-30 23:51:10.000000,2022-07-01 00:07:07.000000,2022-06-30 23:26:28.000000,2022-06-30 23:18:13.000000]]\n",
       "PULocationID: [[234,161,231,87,137,...,97,23,85,72,72],[141,164,138,145,173,...,141,140,168,236,239],...,[248,208,208,107,230,...,161,68,158,158,186],[246,66,252,188,76,...,74,224,231,234,244]]\n",
       "DOLocationID: [[114,151,87,225,162,...,65,23,39,72,89],[237,140,164,174,56,...,75,168,260,238,232],...,[208,208,168,230,161,...,164,37,158,164,246],[48,232,121,76,148,...,224,13,231,48,242]]\n",
       "trip_miles: [[1.5,4.18,2.91,5.45,1.069,...,1.151,0.35,2.211,1.456,1.978],[0.53,2.602,8.81,13.26,1.15,...,2.69,3.81,5.77,1.57,6.96],...,[4.159,2.513,6.552,1.32,0.9,...,1.11,7.06,1.2,1.49,0.97],[0.81,2.443,6.31,5.97,13.53,...,6.07,4.9,0.53,2.85,6.207]]\n",
       "..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "table = pq.read_table('/Users/mahdi/Downloads/fhvhv_tripdata_2022-06.parquet')\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95e46add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.03908724244684"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(table) / 1024 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc4504cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.64825439453125 gigabytes\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import psutil\n",
    "# def print_mem():\n",
    "#     gig = psutil.Process(os.getpid()).memory_info().rss / 1024 ** 3\n",
    "#     print(f\"{gig} gigabytes\")\n",
    "\n",
    "# print_mem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b0e7e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.87973692920059"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('/Users/mahdi/Downloads/fhvhv_tripdata_2022-06.parquet')\n",
    "sys.getsizeof(df) / 1024 / 1024 / 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f4201f",
   "metadata": {},
   "source": [
    "\n",
    "### Apache Arrow Datasets\n",
    "\n",
    "\n",
    "*   Datasets in PyArrow let you work with large tabular data, even when it's larger than your machine's memory\n",
    "*   It offers lazy data access, meaning you don't have to load the entire dataset into memory.\n",
    "*   Datasets support data discovery, partitioning, and compatibility with various file systems like AWS, Google Cloud, and local storage.\n",
    "  * I can read from AWS or Google without having to install anything.\n",
    "\n",
    "* import the dataset library as:\n",
    "\n",
    "```python\n",
    "import pyarrow.dataset as ds\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50e765",
   "metadata": {},
   "source": [
    "### Dataset Overview\n",
    "\n",
    "* Provider: New York City Taxi and Limousine Commission (TLC)\n",
    "* Data hosted on AWS. The URSA-LAB company account.\n",
    "* Contains data on millions of taxi and limousine trips in NYC\n",
    "* Time Period: 2009 to 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00b42c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE 2009/\r\n",
      "                           PRE 2010/\r\n",
      "                           PRE 2011/\r\n",
      "                           PRE 2012/\r\n",
      "                           PRE 2013/\r\n",
      "                           PRE 2014/\r\n",
      "                           PRE 2015/\r\n",
      "                           PRE 2016/\r\n",
      "                           PRE 2017/\r\n",
      "                           PRE 2018/\r\n",
      "                           PRE 2019/\r\n"
     ]
    }
   ],
   "source": [
    "# **Note**: In the AWS S3 listing, \"PRE\" stands for \"prefix,\" essentially representing a folder or directory.\n",
    "\n",
    "!aws s3 ls \"s3://ursa-labs-taxi-data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4c3ab1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE 01/\r\n",
      "                           PRE 02/\r\n",
      "                           PRE 03/\r\n",
      "                           PRE 04/\r\n",
      "                           PRE 05/\r\n",
      "                           PRE 06/\r\n",
      "                           PRE 07/\r\n",
      "                           PRE 08/\r\n",
      "                           PRE 09/\r\n",
      "                           PRE 10/\r\n",
      "                           PRE 11/\r\n",
      "                           PRE 12/\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls \"s3://ursa-labs-taxi-data/2009/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3da9fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 171 ms, sys: 257 ms, total: 428 ms\n",
      "Wall time: 7.54 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyarrow._dataset.FileSystemDataset at 0x1123f6080>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import pyarrow.dataset as ds\n",
    "dataset = ds.dataset(\"s3://ursa-labs-taxi-data/\", partitioning=[\"year\", \"month\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f60be24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdea9bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ursa-labs-taxi-data/2009/01/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/02/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/03/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/04/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/05/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/06/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/07/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/08/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/09/data.parquet',\n",
       " 'ursa-labs-taxi-data/2009/10/data.parquet']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.files[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d2e4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.compute.Expression ((year == 2009) and (month == 1))>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's how to load just one file (a fragment) and its schema:\n",
    "\n",
    "frag = next(dataset.get_fragments())\n",
    "frag.partition_expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d98407",
   "metadata": {},
   "source": [
    "#### Play with a Single File\n",
    "\n",
    "* Let's read in the data from this single fragment\n",
    "* Take a look at the data\n",
    "* List of column names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd4124c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.54 s, sys: 5.97 s, total: 13.5 s\n",
      "Wall time: 1min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "vendor_id: string\n",
       "pickup_at: timestamp[us]\n",
       "dropoff_at: timestamp[us]\n",
       "passenger_count: int8\n",
       "trip_distance: float\n",
       "pickup_longitude: float\n",
       "pickup_latitude: float\n",
       "rate_code_id: null\n",
       "store_and_fwd_flag: string\n",
       "dropoff_longitude: float\n",
       "dropoff_latitude: float\n",
       "payment_type: string\n",
       "fare_amount: float\n",
       "extra: float\n",
       "mta_tax: float\n",
       "tip_amount: float\n",
       "tolls_amount: float\n",
       "total_amount: float\n",
       "----\n",
       "vendor_id: [[\"VTS\",\"VTS\",\"VTS\",\"DDS\",\"DDS\",...,\"DDS\",\"CMT\",\"CMT\",\"CMT\",\"CMT\"],[\"CMT\",\"DDS\",\"DDS\",\"CMT\",\"DDS\",...,\"CMT\",\"CMT\",\"CMT\",\"CMT\",\"CMT\"],...,[\"CMT\",\"CMT\",\"DDS\",\"CMT\",\"CMT\",...,\"VTS\",\"CMT\",\"VTS\",\"VTS\",\"VTS\"],[\"VTS\",\"VTS\",\"VTS\",\"VTS\",\"VTS\",...,\"VTS\",\"VTS\",\"CMT\",\"VTS\",\"CMT\"]]\n",
       "pickup_at: [[2009-01-04 02:52:00.000000,2009-01-04 03:31:00.000000,2009-01-03 15:43:00.000000,2009-01-01 20:52:58.000000,2009-01-24 16:18:23.000000,...,2009-01-01 22:42:49.000000,2009-01-04 18:27:32.000000,2009-01-04 11:48:33.000000,2009-01-04 23:21:04.000000,2009-01-04 16:11:27.000000],[2009-01-04 21:54:44.000000,2009-01-02 15:41:33.000000,2009-01-02 13:20:36.000000,2009-01-04 14:00:03.000000,2009-01-28 13:53:51.000000,...,2009-01-25 10:50:39.000000,2009-01-25 11:49:11.000000,2009-01-21 12:47:20.000000,2009-01-25 13:20:23.000000,2009-01-30 12:53:03.000000],...,[2009-01-21 08:17:06.000000,2009-01-20 20:09:15.000000,2009-01-09 19:14:11.000000,2009-01-20 21:02:47.000000,2009-01-21 08:37:05.000000,...,2009-01-25 00:04:00.000000,2009-01-01 01:30:02.000000,2009-01-25 14:40:00.000000,2009-01-22 20:46:00.000000,2009-01-25 13:07:00.000000],[2009-01-25 06:39:00.000000,2009-01-25 15:00:00.000000,2009-01-25 10:11:00.000000,2009-01-25 04:19:00.000000,2009-01-23 19:59:00.000000,...,2009-01-27 14:36:00.000000,2009-01-27 13:56:00.000000,2009-01-23 08:39:44.000000,2009-01-24 23:05:00.000000,2009-01-23 14:39:02.000000]]\n",
       "dropoff_at: [[2009-01-04 03:02:00.000000,2009-01-04 03:38:00.000000,2009-01-03 15:57:00.000000,2009-01-01 21:14:00.000000,2009-01-24 16:24:56.000000,...,2009-01-01 23:01:49.000000,2009-01-04 18:31:39.000000,2009-01-04 11:54:40.000000,2009-01-04 23:24:16.000000,2009-01-04 16:24:19.000000],[2009-01-04 21:58:25.000000,2009-01-02 15:45:22.000000,2009-01-02 13:32:59.000000,2009-01-04 14:03:02.000000,2009-01-28 14:02:28.000000,...,2009-01-25 10:52:42.000000,2009-01-25 12:07:32.000000,2009-01-21 13:01:08.000000,2009-01-25 13:24:09.000000,2009-01-30 12:59:29.000000],...,[2009-01-21 09:16:34.000000,2009-01-20 20:12:52.000000,2009-01-09 19:27:18.000000,2009-01-20 21:18:31.000000,2009-01-21 08:40:51.000000,...,2009-01-25 00:23:00.000000,2009-01-01 01:49:06.000000,2009-01-25 14:48:00.000000,2009-01-22 20:53:00.000000,2009-01-25 13:20:00.000000],[2009-01-25 06:44:00.000000,2009-01-25 15:04:00.000000,2009-01-25 10:16:00.000000,2009-01-25 04:33:00.000000,2009-01-23 20:14:00.000000,...,2009-01-27 14:46:00.000000,2009-01-27 14:02:00.000000,2009-01-23 09:02:15.000000,2009-01-24 23:15:00.000000,2009-01-23 15:35:15.000000]]\n",
       "passenger_count: [[1,3,5,1,1,...,1,1,1,1,2],[2,2,2,1,2,...,2,2,1,1,1],...,[1,1,1,1,1,...,5,2,5,1,5],[5,1,1,5,1,...,5,1,1,3,1]]\n",
       "trip_distance: [[2.63,4.55,10.35,5,0.4,...,3.7,0.9,0.5,1.1,1.3],[1.1,0.6,0.9,1.2,1,...,1,6.8,1.3,0.7,1],...,[20.7,0.2,2.1,2.3,0.5,...,5.2,4.3,0.86,1.62,1.53],[1.43,0.58,1.36,3.02,2.91,...,0.89,1.94,3.8,3.85,17.3]]\n",
       "pickup_longitude: [[-73.99196,-73.9821,-74.00259,-73.974266,-74.00158,...,-73.992744,-73.99128,-73.98005,-73.97288,-73.9776],[-73.9908,-73.975716,-73.97441,-73.97065,-73.990135,...,-73.97786,-73.95457,0,-73.97422,-73.966644],...,[-73.78204,-73.98393,-73.979645,-73.99633,-73.98092,...,-73.97414,-73.982925,-73.953926,-73.999596,-73.99288],[-73.970604,-73.99453,-73.9917,-74.00047,-73.99772,...,-73.98201,-73.972786,-73.97747,-73.98129,0]]\n",
       "pickup_latitude: [[40.721565,40.73629,40.739746,40.790955,40.719383,...,40.713802,40.723892,40.754658,40.79333,40.752],[40.751076,40.748226,40.76322,40.752747,40.740852,...,40.74217,40.78086,0,40.783566,40.804012],...,[40.644768,40.749287,40.7554,40.737335,40.76395,...,40.78377,40.73503,40.76648,40.733482,40.72419],[40.76505,40.75172,40.739643,40.728706,40.72436,...,40.74333,40.76199,40.75186,40.753,0]]\n",
       "rate_code_id: [65536 nulls,65536 nulls,...,65536 nulls,2173 nulls]\n",
       "store_and_fwd_flag: [[null,null,null,null,null,...,null,null,null,null,null],[null,null,null,null,null,...,null,null,null,null,null],...,[null,null,null,null,null,...,null,null,null,null,null],[null,null,null,null,null,...,null,null,null,null,null]]\n",
       "dropoff_longitude: [[-73.993805,-73.95585,-73.86998,-73.99656,-74.00838,...,-73.98663,-73.98523,-73.98596,-73.981995,-73.99347],[-74.00014,-73.97249,-73.98696,-73.98271,-73.99506,...,-73.98757,-73.93975,0,-73.982,-73.95343],...,[-73.96209,-73.98791,-74.005585,-73.97921,-73.98632,...,-73.917816,-73.96772,-73.96618,-74.00464,-74.00712],[-73.980606,-74.00163,-73.97803,-73.97005,-73.98376,...,-73.99433,-73.95148,-74.00991,-73.949455,0]]\n",
       "..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "frag_table = frag.to_table()\n",
    "frag_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1ed59a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vendor_id',\n",
       " 'pickup_at',\n",
       " 'dropoff_at',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'pickup_longitude',\n",
       " 'pickup_latitude',\n",
       " 'rate_code_id',\n",
       " 'store_and_fwd_flag',\n",
       " 'dropoff_longitude',\n",
       " 'dropoff_latitude',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'total_amount']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frag_table.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53c8e2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14092413"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frag_table.num_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a546d3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "vendor_id: string\n",
       "pickup_at: timestamp[us]\n",
       "dropoff_at: timestamp[us]\n",
       "passenger_count: int8\n",
       "trip_distance: float\n",
       "pickup_longitude: float\n",
       "pickup_latitude: float\n",
       "rate_code_id: null\n",
       "store_and_fwd_flag: string\n",
       "dropoff_longitude: float\n",
       "dropoff_latitude: float\n",
       "payment_type: string\n",
       "fare_amount: float\n",
       "extra: float\n",
       "mta_tax: float\n",
       "tip_amount: float\n",
       "tolls_amount: float\n",
       "total_amount: float\n",
       "----\n",
       "vendor_id: [[\"VTS\",\"VTS\",\"VTS\",\"DDS\",\"DDS\",...,\"DDS\",\"CMT\",\"CMT\",\"CMT\",\"CMT\"],[\"CMT\",\"DDS\",\"DDS\",\"CMT\",\"DDS\",...,\"CMT\",\"CMT\",\"CMT\",\"CMT\",\"CMT\"],...,[\"CMT\",\"CMT\",\"DDS\",\"CMT\",\"CMT\",...,\"VTS\",\"CMT\",\"VTS\",\"VTS\",\"VTS\"],[\"VTS\",\"VTS\",\"VTS\",\"VTS\",\"VTS\",...,\"VTS\",\"VTS\",\"CMT\",\"VTS\",\"CMT\"]]\n",
       "pickup_at: [[2009-01-04 02:52:00.000000,2009-01-04 03:31:00.000000,2009-01-03 15:43:00.000000,2009-01-01 20:52:58.000000,2009-01-24 16:18:23.000000,...,2009-01-01 22:42:49.000000,2009-01-04 18:27:32.000000,2009-01-04 11:48:33.000000,2009-01-04 23:21:04.000000,2009-01-04 16:11:27.000000],[2009-01-04 21:54:44.000000,2009-01-02 15:41:33.000000,2009-01-02 13:20:36.000000,2009-01-04 14:00:03.000000,2009-01-28 13:53:51.000000,...,2009-01-25 10:50:39.000000,2009-01-25 11:49:11.000000,2009-01-21 12:47:20.000000,2009-01-25 13:20:23.000000,2009-01-30 12:53:03.000000],...,[2009-01-21 08:17:06.000000,2009-01-20 20:09:15.000000,2009-01-09 19:14:11.000000,2009-01-20 21:02:47.000000,2009-01-21 08:37:05.000000,...,2009-01-25 00:04:00.000000,2009-01-01 01:30:02.000000,2009-01-25 14:40:00.000000,2009-01-22 20:46:00.000000,2009-01-25 13:07:00.000000],[2009-01-25 06:39:00.000000,2009-01-25 15:00:00.000000,2009-01-25 10:11:00.000000,2009-01-25 04:19:00.000000,2009-01-23 19:59:00.000000,...,2009-01-27 14:36:00.000000,2009-01-27 13:56:00.000000,2009-01-23 08:39:44.000000,2009-01-24 23:05:00.000000,2009-01-23 14:39:02.000000]]\n",
       "dropoff_at: [[2009-01-04 03:02:00.000000,2009-01-04 03:38:00.000000,2009-01-03 15:57:00.000000,2009-01-01 21:14:00.000000,2009-01-24 16:24:56.000000,...,2009-01-01 23:01:49.000000,2009-01-04 18:31:39.000000,2009-01-04 11:54:40.000000,2009-01-04 23:24:16.000000,2009-01-04 16:24:19.000000],[2009-01-04 21:58:25.000000,2009-01-02 15:45:22.000000,2009-01-02 13:32:59.000000,2009-01-04 14:03:02.000000,2009-01-28 14:02:28.000000,...,2009-01-25 10:52:42.000000,2009-01-25 12:07:32.000000,2009-01-21 13:01:08.000000,2009-01-25 13:24:09.000000,2009-01-30 12:59:29.000000],...,[2009-01-21 09:16:34.000000,2009-01-20 20:12:52.000000,2009-01-09 19:27:18.000000,2009-01-20 21:18:31.000000,2009-01-21 08:40:51.000000,...,2009-01-25 00:23:00.000000,2009-01-01 01:49:06.000000,2009-01-25 14:48:00.000000,2009-01-22 20:53:00.000000,2009-01-25 13:20:00.000000],[2009-01-25 06:44:00.000000,2009-01-25 15:04:00.000000,2009-01-25 10:16:00.000000,2009-01-25 04:33:00.000000,2009-01-23 20:14:00.000000,...,2009-01-27 14:46:00.000000,2009-01-27 14:02:00.000000,2009-01-23 09:02:15.000000,2009-01-24 23:15:00.000000,2009-01-23 15:35:15.000000]]\n",
       "passenger_count: [[1,3,5,1,1,...,1,1,1,1,2],[2,2,2,1,2,...,2,2,1,1,1],...,[1,1,1,1,1,...,5,2,5,1,5],[5,1,1,5,1,...,5,1,1,3,1]]\n",
       "trip_distance: [[2.63,4.55,10.35,5,0.4,...,3.7,0.9,0.5,1.1,1.3],[1.1,0.6,0.9,1.2,1,...,1,6.8,1.3,0.7,1],...,[20.7,0.2,2.1,2.3,0.5,...,5.2,4.3,0.86,1.62,1.53],[1.43,0.58,1.36,3.02,2.91,...,0.89,1.94,3.8,3.85,17.3]]\n",
       "pickup_longitude: [[-73.99196,-73.9821,-74.00259,-73.974266,-74.00158,...,-73.992744,-73.99128,-73.98005,-73.97288,-73.9776],[-73.9908,-73.975716,-73.97441,-73.97065,-73.990135,...,-73.97786,-73.95457,0,-73.97422,-73.966644],...,[-73.78204,-73.98393,-73.979645,-73.99633,-73.98092,...,-73.97414,-73.982925,-73.953926,-73.999596,-73.99288],[-73.970604,-73.99453,-73.9917,-74.00047,-73.99772,...,-73.98201,-73.972786,-73.97747,-73.98129,0]]\n",
       "pickup_latitude: [[40.721565,40.73629,40.739746,40.790955,40.719383,...,40.713802,40.723892,40.754658,40.79333,40.752],[40.751076,40.748226,40.76322,40.752747,40.740852,...,40.74217,40.78086,0,40.783566,40.804012],...,[40.644768,40.749287,40.7554,40.737335,40.76395,...,40.78377,40.73503,40.76648,40.733482,40.72419],[40.76505,40.75172,40.739643,40.728706,40.72436,...,40.74333,40.76199,40.75186,40.753,0]]\n",
       "rate_code_id: [65536 nulls,65536 nulls,...,65536 nulls,2173 nulls]\n",
       "store_and_fwd_flag: [[null,null,null,null,null,...,null,null,null,null,null],[null,null,null,null,null,...,null,null,null,null,null],...,[null,null,null,null,null,...,null,null,null,null,null],[null,null,null,null,null,...,null,null,null,null,null]]\n",
       "dropoff_longitude: [[-73.993805,-73.95585,-73.86998,-73.99656,-74.00838,...,-73.98663,-73.98523,-73.98596,-73.981995,-73.99347],[-74.00014,-73.97249,-73.98696,-73.98271,-73.99506,...,-73.98757,-73.93975,0,-73.982,-73.95343],...,[-73.96209,-73.98791,-74.005585,-73.97921,-73.98632,...,-73.917816,-73.96772,-73.96618,-74.00464,-74.00712],[-73.980606,-74.00163,-73.97803,-73.97005,-73.98376,...,-73.99433,-73.95148,-74.00991,-73.949455,0]]\n",
       "..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frag_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626148d",
   "metadata": {},
   "source": [
    "#### Chunks: The Building Blocks\n",
    "\n",
    "* Remember how we talked about Arrow tables having columns that could be split into chunks? \n",
    "* If you take a look, each column is divided into 216 chunks\n",
    "  * Proving that this table is built in the way we discussed earlier.\n",
    "* Take just a slice of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b60f1dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "vendor_id: string\n",
       "pickup_at: timestamp[us]\n",
       "dropoff_at: timestamp[us]\n",
       "passenger_count: int8\n",
       "trip_distance: float\n",
       "pickup_longitude: float\n",
       "pickup_latitude: float\n",
       "rate_code_id: null\n",
       "store_and_fwd_flag: string\n",
       "dropoff_longitude: float\n",
       "dropoff_latitude: float\n",
       "payment_type: string\n",
       "fare_amount: float\n",
       "extra: float\n",
       "mta_tax: float\n",
       "tip_amount: float\n",
       "tolls_amount: float\n",
       "total_amount: float\n",
       "----\n",
       "vendor_id: [[\"VTS\",\"VTS\",\"VTS\",\"DDS\",\"DDS\"]]\n",
       "pickup_at: [[2009-01-04 02:52:00.000000,2009-01-04 03:31:00.000000,2009-01-03 15:43:00.000000,2009-01-01 20:52:58.000000,2009-01-24 16:18:23.000000]]\n",
       "dropoff_at: [[2009-01-04 03:02:00.000000,2009-01-04 03:38:00.000000,2009-01-03 15:57:00.000000,2009-01-01 21:14:00.000000,2009-01-24 16:24:56.000000]]\n",
       "passenger_count: [[1,3,5,1,1]]\n",
       "trip_distance: [[2.63,4.55,10.35,5,0.4]]\n",
       "pickup_longitude: [[-73.99196,-73.9821,-74.00259,-73.974266,-74.00158]]\n",
       "pickup_latitude: [[40.721565,40.73629,40.739746,40.790955,40.719383]]\n",
       "rate_code_id: [5 nulls]\n",
       "store_and_fwd_flag: [[null,null,null,null,null]]\n",
       "dropoff_longitude: [[-73.993805,-73.95585,-73.86998,-73.99656,-74.00838]]\n",
       "..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frag_table.slice(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "423b3adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[frag_table[col_name].num_chunks for col_name in frag_table.column_names]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3e6dd",
   "metadata": {},
   "source": [
    "### The Essentials of Apache Arrow Tables and Record Batches\n",
    "\n",
    "*  Tables in Apache Arrow are essentially collections of record batches.\n",
    "*  You can easily pull data from columns like `payment_type`, `fare_amount`, or `tip_amount`. \n",
    "* Because we're working with a single record batch, managing the data is pretty straightforward. \n",
    "  * We'll see that each column, for instance, holds 65,536 values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf90cabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.RecordBatch\n",
       "vendor_id: string\n",
       "pickup_at: timestamp[us]\n",
       "dropoff_at: timestamp[us]\n",
       "passenger_count: int8\n",
       "trip_distance: float\n",
       "pickup_longitude: float\n",
       "pickup_latitude: float\n",
       "rate_code_id: null\n",
       "store_and_fwd_flag: string\n",
       "dropoff_longitude: float\n",
       "dropoff_latitude: float\n",
       "payment_type: string\n",
       "fare_amount: float\n",
       "extra: float\n",
       "mta_tax: float\n",
       "tip_amount: float\n",
       "tolls_amount: float\n",
       "total_amount: float\n",
       "----\n",
       "vendor_id: [\"DDS\",\"DDS\",\"CMT\",\"CMT\",\"CMT\",\"CMT\",\"CMT\",\"DDS\",\"DDS\",\"CMT\",...,\"VTS\",\"VTS\",\"CMT\",\"VTS\",\"VTS\",\"VTS\",\"VTS\",\"VTS\",\"VTS\",\"VTS\"]\n",
       "pickup_at: [2009-01-12 12:54:37.000000,2009-01-12 17:24:48.000000,2009-01-10 23:45:52.000000,2009-01-10 19:45:10.000000,2009-01-10 23:41:18.000000,2009-01-10 23:41:38.000000,2009-01-31 20:03:56.000000,2009-01-23 13:16:40.000000,2009-01-13 00:09:28.000000,2009-01-29 15:50:01.000000,...,2009-01-03 13:57:00.000000,2009-01-03 22:47:00.000000,2009-01-18 04:10:02.000000,2009-01-03 18:28:00.000000,2009-01-03 15:38:00.000000,2009-01-03 16:19:00.000000,2009-01-03 23:05:00.000000,2009-01-25 07:50:00.000000,2009-01-02 15:28:00.000000,2009-01-04 07:08:00.000000]\n",
       "dropoff_at: [2009-01-12 12:59:22.000000,2009-01-12 17:39:15.000000,2009-01-10 23:48:30.000000,2009-01-10 19:55:32.000000,2009-01-10 23:43:08.000000,2009-01-10 23:58:54.000000,2009-01-31 20:17:13.000000,2009-01-23 13:26:19.000000,2009-01-13 00:13:38.000000,2009-01-29 16:06:47.000000,...,2009-01-03 14:44:00.000000,2009-01-03 22:55:00.000000,2009-01-18 04:14:02.000000,2009-01-03 18:48:00.000000,2009-01-03 15:42:00.000000,2009-01-03 16:46:00.000000,2009-01-03 23:07:00.000000,2009-01-25 08:06:00.000000,2009-01-02 15:36:00.000000,2009-01-04 07:26:00.000000]\n",
       "passenger_count: [1,1,1,3,1,3,1,1,1,1,...,1,2,1,3,1,1,1,1,4,1]\n",
       "trip_distance: [0.8,3.2,0.6,1.6,0.2,6.8,2.5,1.4,0.4,2.4,...,20.53,1.36,1.7,10.71,1.45,10.16,1.35,10.34,1.64,9.36]\n",
       "pickup_longitude: [-73.96626,-73.98048,-73.97805,-73.98807,-73.993126,-73.9546,-74.00847,-73.969955,-73.99127,-73.95605,...,-73.789856,-73.99772,-73.97988,-73.9802,-73.98169,-73.81576,-73.988014,-73.98845,-73.97859,-73.95896]\n",
       "pickup_latitude: [40.76751,40.750954,40.752308,40.74979,40.722385,40.732124,40.704304,40.784714,40.72358,40.76394,...,40.643955,40.720814,40.734673,40.72701,40.73251,40.764023,40.73781,40.731342,40.761646,40.763744]\n",
       "rate_code_id: 65536 nulls\n",
       "store_and_fwd_flag: [null,null,null,null,null,null,null,null,null,null,...,null,null,null,null,null,null,null,null,null,null]\n",
       "dropoff_longitude: [-73.9579,-73.96642,-73.986916,-73.98978,-73.99183,-73.982735,-74.002914,-73.95634,-73.984024,-73.98601,...,-74.00891,-74.016045,-73.96773,-73.86163,-73.97239,-73.81548,-74.0017,-73.871056,-73.97276,-73.91705]\n",
       "..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_batch_3 = frag_table.to_batches()[3]\n",
    "record_batch_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30cbb487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_batch_3.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "136d7fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.FloatArray object at 0x1123f5f00>\n",
       "[\n",
       "  4.9,\n",
       "  10.5,\n",
       "  4.2,\n",
       "  8.2,\n",
       "  3.8,\n",
       "  17.8,\n",
       "  9.8,\n",
       "  6.9,\n",
       "  3.7,\n",
       "  10.5,\n",
       "  ...\n",
       "  45,\n",
       "  6.9,\n",
       "  6.2,\n",
       "  25.3,\n",
       "  5.7,\n",
       "  25.3,\n",
       "  5.3,\n",
       "  24.1,\n",
       "  6.9,\n",
       "  22.1\n",
       "]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_batch_3[\"fare_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0735bc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.FloatArray object at 0x126ac3220>\n",
       "[\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.76,\n",
       "  2.67,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5.06,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0\n",
       "]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_batch_3['tip_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8074fa20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.StringArray object at 0x1766abfa0>\n",
       "[\n",
       "  \"CASH\",\n",
       "  \"CASH\",\n",
       "  \"Cash\",\n",
       "  \"Cash\",\n",
       "  \"Credit\",\n",
       "  \"Credit\",\n",
       "  \"Credit\",\n",
       "  \"CASH\",\n",
       "  \"CASH\",\n",
       "  \"Cash\",\n",
       "  ...\n",
       "  \"CASH\",\n",
       "  \"CASH\",\n",
       "  \"Cash\",\n",
       "  \"Credit\",\n",
       "  \"CASH\",\n",
       "  \"CASH\",\n",
       "  \"Credit\",\n",
       "  \"CASH\",\n",
       "  \"CASH\",\n",
       "  \"CASH\"\n",
       "]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_batch_3['payment_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75535c",
   "metadata": {},
   "source": [
    "#### PyArrow's Computational Capabilities\n",
    "\n",
    "*   PyArrow separates data storage concerns from computational functionality.    \n",
    "    * Structures like Arrow Arrays, Record Batches, and Tables handle data storage and serialization.\n",
    "    * For actual data operations, there's the `pyarrow.compute` module.\n",
    "*   The `pyarrow.compute` module offers a range of functions for filtering, transforming, and aggregating data.    \n",
    "    * While it does provide useful operations, it's not a full-blown analytical tool. \n",
    "    * For more complex tasks, you'd typically use something like Pandas or Spark.\n",
    "\n",
    "* Let's perform some computations like calculating the sum of tips and fares, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c46b7280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.FloatArray object at 0x1766d48e0>\n",
       "[\n",
       "  4.9,\n",
       "  10.5,\n",
       "  4.2,\n",
       "  8.2,\n",
       "  4.56,\n",
       "  20.47,\n",
       "  11.8,\n",
       "  6.9,\n",
       "  3.7,\n",
       "  10.5,\n",
       "  ...\n",
       "  45,\n",
       "  6.9,\n",
       "  6.2,\n",
       "  30.359999,\n",
       "  5.7,\n",
       "  25.3,\n",
       "  6.3,\n",
       "  24.1,\n",
       "  6.9,\n",
       "  22.1\n",
       "]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow.compute as pc\n",
    "pc.add(record_batch_3['tip_amount'], record_batch_3['fare_amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0365b652",
   "metadata": {},
   "source": [
    "* How about finding the maximum total amount for a trip, including the tip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "272d4043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.FloatScalar: 164.0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.max(pc.add(record_batch_3['tip_amount'], record_batch_3['fare_amount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e0bcf",
   "metadata": {},
   "source": [
    "* And the average?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5162427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.DoubleScalar: 10.015554052642983>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.mean(pc.add(record_batch_3['tip_amount'], record_batch_3['fare_amount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a8057",
   "metadata": {},
   "source": [
    "* We can also perform operations on string data, like converting the case of `payment_type`, which has been recorded inconsistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4ea7d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.StringArray object at 0x1766d4520>\n",
       "[\n",
       "  \"CASH\",\n",
       "  \"CASH\",\n",
       "  \"CASH\",\n",
       "  \"CASH\",\n",
       "  \"CREDIT\",\n",
       "  \"CREDIT\",\n",
       "  \"CREDIT\",\n",
       "  \"CASH\",\n",
       "  \"CASH\",\n",
       "  \"CASH\",\n",
       "  ...\n",
       "  \"CASH\",\n",
       "  \"CASH\",\n",
       "  \"CASH\",\n",
       "  \"CREDIT\",\n",
       "  \"CASH\",\n",
       "  \"CASH\",\n",
       "  \"CREDIT\",\n",
       "  \"CASH\",\n",
       "  \"CASH\",\n",
       "  \"CASH\"\n",
       "]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_cased_payment_type = pc.utf8_upper(record_batch_3[\"payment_type\"])\n",
    "upper_cased_payment_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3597a",
   "metadata": {},
   "source": [
    "* You can then filter data based on whether the payment type was \"CASH.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c334143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.BooleanArray object at 0x1766d49a0>\n",
       "[\n",
       "  true,\n",
       "  true,\n",
       "  true,\n",
       "  true,\n",
       "  false,\n",
       "  false,\n",
       "  false,\n",
       "  true,\n",
       "  true,\n",
       "  true,\n",
       "  ...\n",
       "  true,\n",
       "  true,\n",
       "  true,\n",
       "  false,\n",
       "  true,\n",
       "  true,\n",
       "  false,\n",
       "  true,\n",
       "  true,\n",
       "  true\n",
       "]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_cash = pc.equal(upper_cased_payment_type, pa.scalar('CASH'))\n",
    "is_cash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ff03b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51341"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_record_batch_3 = pc.filter(record_batch_3, is_cash)\n",
    "filtered_record_batch_3\n",
    "filtered_record_batch_3.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7912d0",
   "metadata": {},
   "source": [
    "\n",
    "#### Working with Parquet Files\n",
    "\n",
    "* You can read Parquet data into PyArrow as a ParquetDataset, and then work with it as ParquetFile Fragments.\n",
    "* Recall that: \n",
    "    * Each fragment has its own metadata, \n",
    "    * You can also get statistics about each row group within the fragment.\n",
    "      * However, it's usually more efficient to work with sorted data if you carry out frequent operations\n",
    "      * You can then save this sorted table into a new Parquet file for optimized data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb1e6667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.parquet.core._ParquetDatasetV2 at 0x16aa1bd90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow as pa \n",
    "import pyarrow.parquet as pq\n",
    "dataset = pq.ParquetDataset('s3://ursa-labs-taxi-data/2009/', partitioning=[\"month\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13072afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.58 s, sys: 1.58 s, total: 10.2 s\n",
      "Wall time: 1min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.UInt64Array object at 0x179529c60>\n",
       "[\n",
       "  7128666,\n",
       "  8770671,\n",
       "  4811616,\n",
       "  7072796,\n",
       "  9304633,\n",
       "  1333873,\n",
       "  8582999,\n",
       "  2419463,\n",
       "  679644,\n",
       "  2935483,\n",
       "  ...\n",
       "  12352984,\n",
       "  5215411,\n",
       "  8560388,\n",
       "  11583944,\n",
       "  10773005,\n",
       "  11904863,\n",
       "  8725710,\n",
       "  3507901,\n",
       "  6245344,\n",
       "  743581\n",
       "]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "data_table = dataset.fragments[0].to_table() \n",
    "sorted_indices = pc.sort_indices(data_table, sort_keys=[(\"dropoff_at\", \"ascending\"), (\"fare_amount\", \"ascending\")])\n",
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4af0862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the instances in the order specified in the variable sorted_indices\n",
    "# i.e., sorting the data\n",
    "sorted_table = data_table.take(sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "344ee6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pq.write_table(sorted_table, 'optimized_parquet_file.parquet', row_group_size=65536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33953a7",
   "metadata": {},
   "source": [
    "\n",
    "#### Exploring Sorted Parquet Files\n",
    "\n",
    "*   When you read the sorted table back into PyArrow, it's easier to work with.\n",
    "  * We can reach the read groups meta data and only look at those we are interested in.\n",
    "  * i.e., you can delve into the metadata to understand your data better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "836dc2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_parquet_file = pq.ParquetFile('optimized_parquet_file.parquet')\n",
    "rg0_metadata = optimized_parquet_file.metadata.row_group(0)\n",
    "rg0_metadata_dict = rg0_metadata.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c3fd4dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'vendor_id'),\n",
       " (1, 'pickup_at'),\n",
       " (2, 'dropoff_at'),\n",
       " (3, 'passenger_count'),\n",
       " (4, 'trip_distance'),\n",
       " (5, 'pickup_longitude'),\n",
       " (6, 'pickup_latitude'),\n",
       " (7, 'rate_code_id'),\n",
       " (8, 'store_and_fwd_flag'),\n",
       " (9, 'dropoff_longitude'),\n",
       " (10, 'dropoff_latitude'),\n",
       " (11, 'payment_type'),\n",
       " (12, 'fare_amount'),\n",
       " (13, 'extra'),\n",
       " (14, 'mta_tax'),\n",
       " (15, 'tip_amount'),\n",
       " (16, 'tolls_amount'),\n",
       " (17, 'total_amount')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i,x[\"path_in_schema\"]) for i, x in enumerate(rg0_metadata.to_dict()[\"columns\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9f7785bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vendor_id': 0,\n",
       " 'pickup_at': 1,\n",
       " 'dropoff_at': 2,\n",
       " 'passenger_count': 3,\n",
       " 'trip_distance': 4,\n",
       " 'pickup_longitude': 5,\n",
       " 'pickup_latitude': 6,\n",
       " 'rate_code_id': 7,\n",
       " 'store_and_fwd_flag': 8,\n",
       " 'dropoff_longitude': 9,\n",
       " 'dropoff_latitude': 10,\n",
       " 'payment_type': 11,\n",
       " 'fare_amount': 12,\n",
       " 'extra': 13,\n",
       " 'mta_tax': 14,\n",
       " 'tip_amount': 15,\n",
       " 'tolls_amount': 16,\n",
       " 'total_amount': 17}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_2_pos = {x[\"path_in_schema\"]:i for i, x in enumerate(rg0_metadata.to_dict()[\"columns\"])}\n",
    "name_2_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507c394c",
   "metadata": {},
   "source": [
    "### Bonus Question 1\n",
    "    *  can you get the average transaction between 2:00-2:59 PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a464a355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start point (2:00 PM) found in row_group 2\n",
      "Ending point (2:59 PM) found in row_group 2\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "col_idx = name_2_pos['dropoff_at']\n",
    "\n",
    "datetime_obj_start = datetime.strptime(\"2009-1-1 14:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "datetime_obj_end = datetime.strptime(\"2009-1-1 14:59:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "for i in range(optimized_parquet_file.num_row_groups):\n",
    "    col_stats = optimized_parquet_file.metadata.row_group(i).column(col_idx).statistics    \n",
    "    if col_stats.min <= datetime_obj_start <= col_stats.max:\n",
    "        print(f\"Start point (2:00 PM) found in row_group {i}\")\n",
    "        \n",
    "    if col_stats.min <= datetime_obj_end <= col_stats.max:\n",
    "        print(f\"Ending point (2:59 PM) found in row_group {i}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "68c81737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering \"Dropoff_at\" instead of \"pickup_at\" as the transaction takes place after the dropoff\n",
    "\n",
    "rg2 = optimized_parquet_file.read_row_group(2)\n",
    "\n",
    "#Drop-off time >= 2:00 PM\n",
    "gr_2_00PM = pc.greater_equal(rg2['dropoff_at'], datetime_obj_start)\n",
    "\n",
    "#Drop-off time <= 2:59 PM\n",
    "ls_2_59PM = pc.less_equal(rg2['dropoff_at'], datetime_obj_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6fc31eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drop_off_in_range = pc.filter(rg2, pc.and_(gr_2_00PM, ls_2_59PM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a489c392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x179f22e80>\n",
       "[\n",
       "  [\n",
       "    2.5,\n",
       "    2.9,\n",
       "    3.3,\n",
       "    3.3,\n",
       "    3.3,\n",
       "    ...\n",
       "    49.15,\n",
       "    49.15,\n",
       "    49.15,\n",
       "    45,\n",
       "    47.7\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_drop_off_in_range['total_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "26735764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.DoubleScalar: 10.370755473289336>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.mean(all_drop_off_in_range['total_amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea2005",
   "metadata": {},
   "source": [
    "    average transaction between 2:00-2:59 PM on 2009-1-1 was <u><b>$10.37</b></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d50dccb",
   "metadata": {},
   "source": [
    "### Bonus Question 2\n",
    "        * Which day, on average has the highest tip? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "910d2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c719d3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 216/216 [00:46<00:00,  4.63it/s]\n"
     ]
    }
   ],
   "source": [
    "max = 0 \n",
    "max_date = ''\n",
    "\n",
    "for i in tqdm(range(optimized_parquet_file.num_row_groups)):\n",
    "    rg = optimized_parquet_file.read_row_group(i).to_pandas()\n",
    "    rg['dropoff_at'] = rg['dropoff_at'].astype('str')\n",
    "    rg['dropoff_at'] = rg['dropoff_at'].str.split(' ', expand=True)[0]\n",
    "\n",
    "    rg['pickup_at'] = rg['pickup_at'].astype('str')\n",
    "    rg['pickup_at'] = rg['pickup_at'].str.split(' ', expand=True)[0]\n",
    "    avg_tip = rg.groupby('pickup_at')['tip_amount'].mean()\n",
    "    if avg_tip.max() > max:\n",
    "        max = avg_tip.max()\n",
    "        max_date = avg_tip.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "08dc098d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.34, '2009-01-20')"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max, max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "74999209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 216/216 [00:46<00:00,  4.68it/s]\n"
     ]
    }
   ],
   "source": [
    "tip_per_day = defaultdict(float)\n",
    "\n",
    "for i in tqdm(range(optimized_parquet_file.num_row_groups)):\n",
    "    rg = optimized_parquet_file.read_row_group(i).to_pandas()\n",
    "    rg['dropoff_at'] = rg['dropoff_at'].astype('str')\n",
    "    rg['dropoff_at'] = rg['dropoff_at'].str.split(' ', expand=True)[0]\n",
    "\n",
    "    rg['pickup_at'] = rg['pickup_at'].astype('str')\n",
    "    rg['pickup_at'] = rg['pickup_at'].str.split(' ', expand=True)[0]\n",
    "    \n",
    "    total_tip = rg.groupby('pickup_at')['tip_amount'].sum()\n",
    "\n",
    "    for date, tip in total_tip.items():\n",
    "        tip_per_day[date] += tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "7f532822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2009-01-30', 267957.1112399101),\n",
       " ('2009-01-29', 267373.54067611694),\n",
       " ('2009-01-16', 259469.09983444214),\n",
       " ('2009-01-22', 255874.0814113617),\n",
       " ('2009-01-23', 254119.05178260803),\n",
       " ('2009-01-15', 251352.5110092163),\n",
       " ('2009-01-14', 246874.59818458557),\n",
       " ('2009-01-21', 242664.74174791574),\n",
       " ('2009-01-27', 235145.651512146),\n",
       " ('2009-01-31', 234567.13916015625),\n",
       " ('2009-01-24', 233566.28106117249),\n",
       " ('2009-01-28', 233095.35934638977),\n",
       " ('2009-01-09', 232536.03957176208),\n",
       " ('2009-01-25', 220655.78765583038),\n",
       " ('2009-01-08', 220165.20137023926),\n",
       " ('2009-01-20', 216461.0501509905),\n",
       " ('2009-01-13', 216002.35802078247),\n",
       " ('2009-01-17', 212806.7802324295),\n",
       " ('2009-01-26', 210454.42921829224),\n",
       " ('2009-01-10', 196560.6111415103),\n",
       " ('2009-01-12', 195425.52257466316),\n",
       " ('2009-01-11', 191854.22960281372),\n",
       " ('2009-01-18', 189843.75176525116),\n",
       " ('2009-01-06', 185202.14712047577),\n",
       " ('2009-01-19', 173412.32271385193),\n",
       " ('2009-01-07', 168789.03913497925),\n",
       " ('2009-01-04', 166291.08195114136),\n",
       " ('2009-01-05', 162921.387468338),\n",
       " ('2009-01-03', 158717.58736801147),\n",
       " ('2009-01-02', 128045.41765785217),\n",
       " ('2009-01-01', 108082.29880523682)]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the dictionary by value\n",
    "sorted_tip_per_day = sorted(tip_per_day.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_tip_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "54e5dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = optimized_parquet_file.read_row_group(213).to_pandas()\n",
    "rg['dropoff_at'] = rg['dropoff_at'].astype('str')\n",
    "rg['dropoff_at'] = rg['dropoff_at'].str.split(' ', expand=True)[0]\n",
    "\n",
    "rg['pickup_at'] = rg['pickup_at'].astype('str')\n",
    "rg['pickup_at'] = rg['pickup_at'].str.split(' ', expand=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "4e69cfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2009-01-30'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg['pickup_at'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "07323e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_at</th>\n",
       "      <th>dropoff_at</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>rate_code_id</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [vendor_id, pickup_at, dropoff_at, passenger_count, trip_distance, pickup_longitude, pickup_latitude, rate_code_id, store_and_fwd_flag, dropoff_longitude, dropoff_latitude, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, total_amount]\n",
       "Index: []"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg[rg['pickup_at'] == '2009-01-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a203d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "* Which time (hour) of the day has the highest tip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d501b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4d45c6c",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "1.  [Apache Arrow Homepage](https://arrow.apache.org/)\n",
    "2.  [PyArrow Documentation](https://arrow.apache.org/docs/python/)\n",
    "3.  [PyArrow GitHub Repository](https://github.com/apache/arrow/tree/master/python/pyarrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7262ab3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
