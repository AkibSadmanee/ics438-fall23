{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0eb47fc",
   "metadata": {},
   "source": [
    "### Introduction to PyArrow\n",
    "\n",
    "*   PyArrow serves as a cross-language development environment specifically designed for in-memory data.\n",
    "*   Its primary goal is to boost the performance of analytics applications.\n",
    "*   Emerging from the Apache Arrow project, PyArrow aims to make data interoperability better across different languages and systems.\n",
    "*   It uses an in-memory columnar data representation, offering an optimized memory footprint for complex data structures.\n",
    "*   With zero-copy reads, it facilitates quick data sharing between Python and other languages, sidestepping the need for serialization.\n",
    "*   It supports schemas and metadata, providing data structures that are rich and self-describing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3013192",
   "metadata": {},
   "source": [
    "### PyArrow and Parquet\n",
    "\n",
    "*   PyArrow offers seamless reading and writing operations for Parquet files.\n",
    "*   With column pruning, you can selectively read only the necessary columns from a Parquet file, reducing I/O time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq \n",
    "table = pq.read_table('your_file.parquet', columns=['column1', 'column2']) \n",
    "# Potentially conver the file to pandas if needed for more sophisticated splicing and dicing.\n",
    "df = table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46b332",
   "metadata": {},
   "source": [
    "### Apache Arrow\n",
    "\n",
    "```The core feature of Apache Arrow is its in-memory columnar format. This language-agnostic standard is designed to store structured, table-like datasets efficiently in memory. The data format supports a rich set of data types, including nested and user-defined types, making it suitable for analytic databases, data frame libraries, and more.``` \n",
    "\n",
    "The Apache Arrow Project\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebcdbec",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://blog.djnavarro.net/posts/2021-11-19_starting-apache-arrow-in-r/img/with_arrow.jpg\" width=700>\n",
    "</div>\n",
    "\n",
    "[picture source](https://blog.djnavarro.net/posts/2021-11-19_starting-apache-arrow-in-r/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyarrow`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac5bc2",
   "metadata": {},
   "source": [
    "### PyArrow Data Structures\n",
    "\n",
    "*   PyArrow offers a suite of low-level data structures and methods optimized for both speed and flexibility.\n",
    "*   These structures can be used seamlessly across multiple languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57327800",
   "metadata": {},
   "source": [
    "### Arrow Array\n",
    "\n",
    "*   An Arrow Array is essentially a column of data stored in an efficient, contiguous block of memory.\n",
    "*   Unlike Python lists, these arrays are optimized for high-speed operations and can be transferred across languages without incurring serialization costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22df9c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "arrow_array = pa.array([1, 2, 3, 4, 5])\n",
    "print(type(arrow_array))\n",
    "print(\"---------\")\n",
    "print(arrow_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bcda45",
   "metadata": {},
   "source": [
    "### Arrow Buffer\n",
    "\n",
    "* While not a data structure per se, Arrow Buffers are pivotal in understanding Arrow functionality.\n",
    "* Buffers are blocks of memory that house the data for Arrow Arrays, contributing to efficient storage.\n",
    "* You can even access the buffer's content directly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35754c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = arrow_array.buffers()[1]\n",
    "print(buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c81af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_data = buffer.to_pybytes()\n",
    "print(byte_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed66a0",
   "metadata": {},
   "source": [
    "### Arrow Buffer - Cont'd\n",
    "\n",
    "* Here, the buffer's data contains 40 bytes, each 8 bytes representing an `int64` value for each of the 5 elements in the array.\n",
    "* You can use this buffer data to create a new NumPy array, showing that Arrow and NumPy can share memory.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d462d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "numpy_array = np.frombuffer(buffer, dtype=np.int64)\n",
    "numpy_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983cb2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shares_memory(arrow_array, numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac880c9",
   "metadata": {},
   "source": [
    "### Arrow Buffer - Cont'd\n",
    "\n",
    "* Both `arrow_array` and `numpy_array` share the same underlying data, demonstrating the concept of zero-copy.\n",
    "* You can confirm this by modifying a value in one array and seeing the change in the other.\n",
    "  * Both arrays will now show the updated value.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6df885",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array[1] = 0\n",
    "numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78887976",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b45749",
   "metadata": {},
   "source": [
    "### Schema\n",
    "\n",
    "* A schema in PyArrow defines the structure, column names, and types for Arrow Arrays.\n",
    "* Schemas are crucial as they set the framework for data manipulation and operations in Arrow.\n",
    "  * Give Arrow an idea on how to encode the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = pa.schema([('column1', pa.int64()), ('column2', pa.string())])\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd46394",
   "metadata": {},
   "source": [
    "### Chunked Array\n",
    "\n",
    "*   A Chunked Array in PyArrow is like a single Arrow Array but divided into smaller \"chunks.\"\n",
    "*   This structure allows for the storage and processing of datasets that are too large to fit in memory.\n",
    "*   It's commonly used in distributed computing frameworks and data streaming scenarios.\n",
    "\n",
    "* For example:\n",
    "  * you could have data sent in chunks to optimize throughput\n",
    "  * you might have multiple nodes in a distributed system each producing Arrow Arrays that are collected and represented as a ChunkedArray by the master node.\n",
    "\n",
    "* From a user perspective, a Chunked Array appears as a contiguous sequence of data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_node_1 = pa.array([0,1,2,3,4])\n",
    "results_node_2 = pa.array([5,6,7,8,9,10])\n",
    "chunked_array = pa.chunked_array([results_node_1, results_node_2])\n",
    "chunked_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be520fe2",
   "metadata": {},
   "source": [
    "### Chunked Array - Cont'd\n",
    "\n",
    "* You can index into a single position or even across multiple chunks, making the data handling more versatile.\n",
    "* You can also access individual chunks, allowing for parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d579f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_array[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356cedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_array.chunk(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948c663",
   "metadata": {},
   "source": [
    "### Table\n",
    "\n",
    "* A Table in PyArrow is a container for multiple Arrow ChunkedArrays with a common schema.\n",
    "* Each column in the Table is an Arrow ChunkedArray, and all columns share the same length.\n",
    "* Tables offer an ideal format for handling data in the form of a dataframe.\n",
    "* Tables can also be partitioned across multiple files for large-scale storage, or to be sent across a network, or even to be stored in-memory on a single machine.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "column1 = pa.array([0, 1, 2, 3, 4]) \n",
    "column2 = pa.array(['a', 'b', 'c', 'd', 'e'])\n",
    "table = pa.table({'column1': column1, 'column2': column2})  \n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdaa066",
   "metadata": {},
   "source": [
    "### Record Batch\n",
    "\n",
    "*   A Record Batch is a collection of Arrow Arrays (columns) with the same length, all of which are bundled together with a schema.\n",
    "*   Much like a Chunked Array is a collection of Arrow Arrays, a Table in Apache Arrow is a collection of Record Batches.\n",
    "\n",
    "* Conceptual Relationship\n",
    "  *   In Apache Arrow, the concept of a Record Batch is to a Table what an Arrow Array is to a Chunked Array.\n",
    "    *   Arrays can be grouped together to form a Chunked Array.\n",
    "    *   Record Batches can be grouped together to form a Table.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec7454b",
   "metadata": {},
   "source": [
    "### Record Batch - Cont'd\n",
    "\n",
    "* Use Cases\n",
    "  *   The choice between using a Record Batch or a Table often depends on your specific needs. E.g.:\n",
    "    \n",
    "  *  Streaming Data: If you need to process data on-the-fly, perhaps in a streaming application where you want to process each chunk as it arrives, Record Batches are a good choice.\n",
    "    *   You can serialize and process each Record Batch independently as they arrive, without having to wait for the entire data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column1_array = pa.array([1, 2, 3, 4, 5])\n",
    "column2_array = pa.array(['a', 'b', 'c', 'd', 'e'])\n",
    "schema = pa.schema([('column1', pa.int64()), ('column2', pa.string())])\n",
    "\n",
    "record_batch = pa.record_batch([column1_array, column2_array], schema=schema)\n",
    "record_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18702115",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe3e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch[\"column1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f94f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column1_array_new = pa.array([6, 7, 8, 9, 10])\n",
    "column2_array_new = pa.array(['f', 'g', 'h', 'i', 'j'])\n",
    "record_batch_new = pa.record_batch([column1_array_new, column2_array_new], schema=schema)\n",
    "\n",
    "\n",
    "table = pa.Table.from_batches([record_batch, record_batch_new], schema=schema)\n",
    "table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf06e41",
   "metadata": {},
   "source": [
    "### Record Batch - Cont'd\n",
    "\n",
    "* In the example above, two Record Batches are combined to create a single Table. \n",
    "  * This is analogous to how individual Arrow Arrays can be combined to create a Chunked Array\n",
    "  * Reinforces the idea that a Record Batch is to a Table what an Arrow Array is to a Chunked Array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede8f6b",
   "metadata": {},
   "source": [
    "### Dive Into Real Data: Parquet and Memory Efficiency\n",
    "\n",
    "1.  Let's get hands-on and read a Parquet file using Apache Arrow.\n",
    "2.  Take note: the size of the data when using PyArrow is substantially smaller than a Pandas DataFrame for the same data.\n",
    "3.  Think of this as a little teaser to whet your appetite for data science goodness.\n",
    "\n",
    "**Note**: Here, I'm using the `parquet` module from the PyArrow package. This module knows how to read Parquet files among other things.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaeb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "table = pq.read_table('/Users/mahdi/Downloads/fhvhv_tripdata_2022-06.parquet')\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e46add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(table) / 1024 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4504cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import psutil\n",
    "# def print_mem():\n",
    "#     gig = psutil.Process(os.getpid()).memory_info().rss / 1024 ** 3\n",
    "#     print(f\"{gig} gigabytes\")\n",
    "\n",
    "# print_mem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('/Users/mahdi/Downloads/fhvhv_tripdata_2022-06.parquet')\n",
    "sys.getsizeof(df) / 1024 / 1024 / 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f4201f",
   "metadata": {},
   "source": [
    "\n",
    "### Apache Arrow Datasets\n",
    "\n",
    "\n",
    "*   Datasets in PyArrow let you work with large tabular data, even when it's larger than your machine's memory\n",
    "*   It offers lazy data access, meaning you don't have to load the entire dataset into memory.\n",
    "*   Datasets support data discovery, partitioning, and compatibility with various file systems like AWS, Google Cloud, and local storage.\n",
    "  * I can read from AWS or Google without having to install anything.\n",
    "\n",
    "* import the dataset library as:\n",
    "\n",
    "```python\n",
    "import pyarrow.dataset as ds\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50e765",
   "metadata": {},
   "source": [
    "### Dataset Overview\n",
    "\n",
    "* Provider: New York City Taxi and Limousine Commission (TLC)\n",
    "* Data hosted on AWS. The URSA-LAB company account.\n",
    "* Contains data on millions of taxi and limousine trips in NYC\n",
    "* Time Period: 2009 to 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b42c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Note**: In the AWS S3 listing, \"PRE\" stands for \"prefix,\" essentially representing a folder or directory.\n",
    "\n",
    "!aws s3 ls \"s3://ursa-labs-taxi-data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c3ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls \"s3://ursa-labs-taxi-data/2009/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da9fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pyarrow.dataset as ds\n",
    "dataset = ds.dataset(\"s3://ursa-labs-taxi-data/\", partitioning=[\"year\", \"month\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60be24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.files[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how to load just one file (a fragment) and its schema:\n",
    "\n",
    "frag = next(dataset.get_fragments())\n",
    "frag.partition_expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d98407",
   "metadata": {},
   "source": [
    "#### Play with a Single File\n",
    "\n",
    "* Let's read in the data from this single fragment\n",
    "* Take a look at the data\n",
    "* List of column names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4124c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "frag_table = frag.to_table()\n",
    "frag_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed59a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_table.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_table.num_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626148d",
   "metadata": {},
   "source": [
    "#### Chunks: The Building Blocks\n",
    "\n",
    "* Remember how we talked about Arrow tables having columns that could be split into chunks? \n",
    "* If you take a look, each column is divided into 216 chunks\n",
    "  * Proving that this table is built in the way we discussed earlier.\n",
    "* Take just a slice of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_table.slice(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[frag_table[col_name].num_chunks for col_name in frag_table.column_names]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3e6dd",
   "metadata": {},
   "source": [
    "### The Essentials of Apache Arrow Tables and Record Batches\n",
    "\n",
    "*  Tables in Apache Arrow are essentially collections of record batches.\n",
    "*  You can easily pull data from columns like `payment_type`, `fare_amount`, or `tip_amount`. \n",
    "* Because we're working with a single record batch, managing the data is pretty straightforward. \n",
    "  * We'll see that each column, for instance, holds 65,536 values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf90cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch_3 = frag_table.to_batches()[3]\n",
    "record_batch_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cbb487",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch_3.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d7fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch_3[\"fare_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0735bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch_3['tip_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8074fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_batch_3['payment_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75535c",
   "metadata": {},
   "source": [
    "#### PyArrow's Computational Capabilities\n",
    "\n",
    "*   PyArrow separates data storage concerns from computational functionality.    \n",
    "    * Structures like Arrow Arrays, Record Batches, and Tables handle data storage and serialization.\n",
    "    * For actual data operations, there's the `pyarrow.compute` module.\n",
    "*   The `pyarrow.compute` module offers a range of functions for filtering, transforming, and aggregating data.    \n",
    "    * While it does provide useful operations, it's not a full-blown analytical tool. \n",
    "    * For more complex tasks, you'd typically use something like Pandas or Spark.\n",
    "\n",
    "* Let's perform some computations like calculating the sum of tips and fares, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.compute as pc\n",
    "pc.add(record_batch_3['tip_amount'], record_batch_3['fare_amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0365b652",
   "metadata": {},
   "source": [
    "* How about finding the maximum total amount for a trip, including the tip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.max(pc.add(record_batch_3['tip_amount'], record_batch_3['fare_amount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e0bcf",
   "metadata": {},
   "source": [
    "* And the average?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.mean(pc.add(record_batch_3['tip_amount'], record_batch_3['fare_amount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a8057",
   "metadata": {},
   "source": [
    "* We can also perform operations on string data, like converting the case of `payment_type`, which has been recorded inconsistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_cased_payment_type = pc.utf8_upper(record_batch_3[\"payment_type\"])\n",
    "upper_cased_payment_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3597a",
   "metadata": {},
   "source": [
    "* You can then filter data based on whether the payment type was \"CASH.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c334143",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cash = pc.equal(upper_cased_payment_type, pa.scalar('CASH'))\n",
    "is_cash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff03b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_record_batch_3 = pc.filter(record_batch_3, is_cash)\n",
    "filtered_record_batch_3\n",
    "filtered_record_batch_3.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7912d0",
   "metadata": {},
   "source": [
    "\n",
    "#### Working with Parquet Files\n",
    "\n",
    "* You can read Parquet data into PyArrow as a ParquetDataset, and then work with it as ParquetFile Fragments.\n",
    "* Recall that: \n",
    "    * Each fragment has its own metadata, \n",
    "    * You can also get statistics about each row group within the fragment.\n",
    "      * However, it's usually more efficient to work with sorted data if you carry out frequent operations\n",
    "      * You can then save this sorted table into a new Parquet file for optimized data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e6667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa \n",
    "import pyarrow.parquet as pq\n",
    "dataset = pq.ParquetDataset('s3://ursa-labs-taxi-data/2009/', partitioning=[\"month\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13072afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "data_table = dataset.fragments[0].to_table() \n",
    "sorted_indices = pc.sort_indices(data_table, sort_keys=[(\"dropoff_at\", \"ascending\"), (\"fare_amount\", \"ascending\")])\n",
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af0862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the instances in the order specified in the variable sorted_indices\n",
    "# i.e., sorting the data\n",
    "sorted_table = data_table.take(sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ee6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pq.write_table(sorted_table, 'optimized_parquet_file.parquet', row_group_size=65536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33953a7",
   "metadata": {},
   "source": [
    "\n",
    "#### Exploring Sorted Parquet Files\n",
    "\n",
    "*   When you read the sorted table back into PyArrow, it's easier to work with.\n",
    "  * We can reach the read groups meta data and only look at those we are interested in.\n",
    "  * i.e., you can delve into the metadata to understand your data better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836dc2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_parquet_file = pq.ParquetFile('optimized_parquet_file.parquet')\n",
    "rg0_metadata = optimized_parquet_file.metadata.row_group(0)\n",
    "rg0_metadata_dict = rg0_metadata.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,x[\"path_in_schema\"]) for i, x in enumerate(rg0_metadata.to_dict()[\"columns\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7785bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_2_pos = {x[\"path_in_schema\"]:i for i, x in enumerate(rg0_metadata.to_dict()[\"columns\"])}\n",
    "name_2_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61055ae7",
   "metadata": {},
   "source": [
    "# Bonus Question Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc677fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa \n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d2c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_parquet_file = pq.ParquetFile('optimized_parquet_file.parquet')\n",
    "rg0_metadata = optimized_parquet_file.metadata.row_group(0)\n",
    "name_2_pos = {x[\"path_in_schema\"]:i for i, x in enumerate(rg0_metadata.to_dict()[\"columns\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a464a355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start point (2:00 PM) found in row_group 2\n",
      "Ending point (2:59 PM) found in row_group 2\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "col_idx = name_2_pos['dropoff_at']\n",
    "\n",
    "datetime_obj_start = datetime.strptime(\"2009-1-1 14:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "datetime_obj_end = datetime.strptime(\"2009-1-1 14:59:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "for i in range(optimized_parquet_file.num_row_groups):\n",
    "    col_stats = optimized_parquet_file.metadata.row_group(i).column(col_idx).statistics    \n",
    "    if col_stats.min <= datetime_obj_start <= col_stats.max:\n",
    "        print(f\"Start point (2:00 PM) found in row_group {i}\")\n",
    "        \n",
    "    if col_stats.min <= datetime_obj_end <= col_stats.max:\n",
    "        print(f\"Ending point (2:59 PM) found in row_group {i}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507c394c",
   "metadata": {},
   "source": [
    "### Bonus Question 1\n",
    "    *  can you get the average transaction between 2:00-2:59 PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c81737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering \"Dropoff_at\" instead of \"pickup_at\" as the transaction takes place after the dropoff\n",
    "\n",
    "rg2 = optimized_parquet_file.read_row_group(2)\n",
    "\n",
    "#Drop-off time >= 2:00 PM\n",
    "gr_2_00PM = pc.greater_equal(rg2['dropoff_at'], datetime_obj_start)\n",
    "\n",
    "#Drop-off time <= 2:59 PM\n",
    "ls_2_59PM = pc.less_equal(rg2['dropoff_at'], datetime_obj_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc31eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the data based on the above two conditions\n",
    "all_drop_off_in_range = pc.filter(rg2, pc.and_(gr_2_00PM, ls_2_59PM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a489c392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x12f63f380>\n",
       "[\n",
       "  [\n",
       "    2.5,\n",
       "    2.9,\n",
       "    3.3,\n",
       "    3.3,\n",
       "    3.3,\n",
       "    ...\n",
       "    49.15,\n",
       "    49.15,\n",
       "    49.15,\n",
       "    45,\n",
       "    47.7\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_drop_off_in_range['total_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26735764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.DoubleScalar: 10.370755473289336>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.mean(all_drop_off_in_range['total_amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea2005",
   "metadata": {},
   "source": [
    "    average transaction between 2:00-2:59 PM on 2009-1-1 was $10.37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b6687",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d50dccb",
   "metadata": {},
   "source": [
    "### Bonus Question 2\n",
    "        * Which day, on average has the highest tip? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "910d2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a552b",
   "metadata": {},
   "source": [
    "I consider `Drop Off Day` as tip is given after dropoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ce0d6a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:44<00:00,  4.82it/s]\n"
     ]
    }
   ],
   "source": [
    "tips_sum = [0] * 7\n",
    "tips_count = [0] * 7\n",
    "\n",
    "for i in tqdm(range(optimized_parquet_file.num_row_groups)):\n",
    "    rg = optimized_parquet_file.read_row_group(i, columns=['dropoff_at', 'tip_amount'])\n",
    "\n",
    "    for each_row in range(rg.num_rows):\n",
    "        tip = rg.column('tip_amount')[each_row].as_py()\n",
    "        day = rg.column('dropoff_at')[each_row].as_py().weekday() \n",
    "        tips_sum[day] += tip\n",
    "        tips_count[day] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4e5d4162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips_mean = list(np.array(tips_sum) / np.array(tips_count))\n",
    "\n",
    "max_tip_mean = tips_mean.index(max(tips_mean))\n",
    "max_tip_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "980c7fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tip_sum = tips_sum.index(max(tips_sum))\n",
    "max_tip_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb5d3e",
   "metadata": {},
   "source": [
    "    Considering average tips, Week Day Number 2 (indexed at 0) has the highest tips\n",
    "    Considering sum of tips,, Week Day Number 4 (indexed at 0) has the highest tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e03fa",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c2f721",
   "metadata": {},
   "source": [
    "### Bonus Question 3\n",
    "    * Which time (hour) of the day has the highest tip?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538e5edc",
   "metadata": {},
   "source": [
    "I consider `Drop Off Time` as tip is given after dropoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4a3c027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:44<00:00,  4.86it/s]\n"
     ]
    }
   ],
   "source": [
    "tips_sum = [0] * 24\n",
    "tips_count = [0] * 24\n",
    "\n",
    "for i in tqdm(range(optimized_parquet_file.num_row_groups)):\n",
    "    rg = optimized_parquet_file.read_row_group(i, columns=['dropoff_at', 'tip_amount'])\n",
    "\n",
    "    for each_row in range(rg.num_rows):\n",
    "        tip = rg.column('tip_amount')[each_row].as_py()\n",
    "        hour = rg.column('dropoff_at')[each_row].as_py().hour\n",
    "        tips_sum[hour] += tip\n",
    "        tips_count[hour] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "58ebaa8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips_mean = list(np.array(tips_sum) / np.array(tips_count))\n",
    "\n",
    "max_tip_mean = tips_mean.index(max(tips_mean))\n",
    "max_tip_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "06530b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tip_sum = tips_sum.index(max(tips_sum))\n",
    "max_tip_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab857ca",
   "metadata": {},
   "source": [
    "    Considering average tips, hour index 5 has the highest tips\n",
    "    Considering sum of tips, hour index 19 has the highest tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad5aba9",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d45c6c",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "1.  [Apache Arrow Homepage](https://arrow.apache.org/)\n",
    "2.  [PyArrow Documentation](https://arrow.apache.org/docs/python/)\n",
    "3.  [PyArrow GitHub Repository](https://github.com/apache/arrow/tree/master/python/pyarrow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
